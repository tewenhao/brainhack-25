{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d4c94-f0ea-479c-8ca4-7b0c7449b58c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ca711",
   "metadata": {},
   "source": [
    "Changing Ultralytics conversion function to use 0 indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c338b4fc-e09f-499e-bb6f-3cc92ff761ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from ultralytics.utils import DATASETS_DIR, LOGGER, NUM_THREADS, TQDM\n",
    "from ultralytics.utils.downloads import download, zip_directory\n",
    "from ultralytics.utils.files import increment_path\n",
    "\n",
    "\n",
    "def coco91_to_coco80_class():\n",
    "    \"\"\"\n",
    "    Converts 91-index COCO class IDs to 80-index COCO class IDs.\n",
    "\n",
    "    Returns:\n",
    "        (list): A list of 91 class IDs where the index represents the 80-index class ID and the value is the\n",
    "            corresponding 91-index class ID.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8,\n",
    "        9,\n",
    "        10,\n",
    "        None,\n",
    "        11,\n",
    "        12,\n",
    "        13,\n",
    "        14,\n",
    "        15,\n",
    "        16,\n",
    "        17,\n",
    "        18,\n",
    "        19,\n",
    "        20,\n",
    "        21,\n",
    "        22,\n",
    "        23,\n",
    "        None,\n",
    "        24,\n",
    "        25,\n",
    "        None,\n",
    "        None,\n",
    "        26,\n",
    "        27,\n",
    "        28,\n",
    "        29,\n",
    "        30,\n",
    "        31,\n",
    "        32,\n",
    "        33,\n",
    "        34,\n",
    "        35,\n",
    "        36,\n",
    "        37,\n",
    "        38,\n",
    "        39,\n",
    "        None,\n",
    "        40,\n",
    "        41,\n",
    "        42,\n",
    "        43,\n",
    "        44,\n",
    "        45,\n",
    "        46,\n",
    "        47,\n",
    "        48,\n",
    "        49,\n",
    "        50,\n",
    "        51,\n",
    "        52,\n",
    "        53,\n",
    "        54,\n",
    "        55,\n",
    "        56,\n",
    "        57,\n",
    "        58,\n",
    "        59,\n",
    "        None,\n",
    "        60,\n",
    "        None,\n",
    "        None,\n",
    "        61,\n",
    "        None,\n",
    "        62,\n",
    "        63,\n",
    "        64,\n",
    "        65,\n",
    "        66,\n",
    "        67,\n",
    "        68,\n",
    "        69,\n",
    "        70,\n",
    "        71,\n",
    "        72,\n",
    "        None,\n",
    "        73,\n",
    "        74,\n",
    "        75,\n",
    "        76,\n",
    "        77,\n",
    "        78,\n",
    "        79,\n",
    "        None,\n",
    "    ]\n",
    "\n",
    "\n",
    "def coco80_to_coco91_class():\n",
    "    r\"\"\"\n",
    "    Converts 80-index (val2014) to 91-index (paper).\n",
    "    For details see https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/.\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.loadtxt(\"data/coco.names\", dtype=\"str\", delimiter=\"\\n\")\n",
    "        >>> b = np.loadtxt(\"data/coco_paper.names\", dtype=\"str\", delimiter=\"\\n\")\n",
    "\n",
    "        Convert the darknet to COCO format\n",
    "        >>> x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]\n",
    "\n",
    "        Convert the COCO to darknet format\n",
    "        >>> x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8,\n",
    "        9,\n",
    "        10,\n",
    "        11,\n",
    "        13,\n",
    "        14,\n",
    "        15,\n",
    "        16,\n",
    "        17,\n",
    "        18,\n",
    "        19,\n",
    "        20,\n",
    "        21,\n",
    "        22,\n",
    "        23,\n",
    "        24,\n",
    "        25,\n",
    "        27,\n",
    "        28,\n",
    "        31,\n",
    "        32,\n",
    "        33,\n",
    "        34,\n",
    "        35,\n",
    "        36,\n",
    "        37,\n",
    "        38,\n",
    "        39,\n",
    "        40,\n",
    "        41,\n",
    "        42,\n",
    "        43,\n",
    "        44,\n",
    "        46,\n",
    "        47,\n",
    "        48,\n",
    "        49,\n",
    "        50,\n",
    "        51,\n",
    "        52,\n",
    "        53,\n",
    "        54,\n",
    "        55,\n",
    "        56,\n",
    "        57,\n",
    "        58,\n",
    "        59,\n",
    "        60,\n",
    "        61,\n",
    "        62,\n",
    "        63,\n",
    "        64,\n",
    "        65,\n",
    "        67,\n",
    "        70,\n",
    "        72,\n",
    "        73,\n",
    "        74,\n",
    "        75,\n",
    "        76,\n",
    "        77,\n",
    "        78,\n",
    "        79,\n",
    "        80,\n",
    "        81,\n",
    "        82,\n",
    "        84,\n",
    "        85,\n",
    "        86,\n",
    "        87,\n",
    "        88,\n",
    "        89,\n",
    "        90,\n",
    "    ]\n",
    "\n",
    "\n",
    "def convert_coco(\n",
    "    labels_dir=\"../coco/annotations/\",\n",
    "    save_dir=\"coco_converted/\",\n",
    "    use_segments=False,\n",
    "    use_keypoints=False,\n",
    "    cls91to80=True,\n",
    "    lvis=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts COCO dataset annotations to a YOLO annotation format suitable for training YOLO models.\n",
    "\n",
    "    Args:\n",
    "        labels_dir (str, optional): Path to directory containing COCO dataset annotation files.\n",
    "        save_dir (str, optional): Path to directory to save results to.\n",
    "        use_segments (bool, optional): Whether to include segmentation masks in the output.\n",
    "        use_keypoints (bool, optional): Whether to include keypoint annotations in the output.\n",
    "        cls91to80 (bool, optional): Whether to map 91 COCO class IDs to the corresponding 80 COCO class IDs.\n",
    "        lvis (bool, optional): Whether to convert data in lvis dataset way.\n",
    "\n",
    "    Examples:\n",
    "        >>> from ultralytics.data.converter import convert_coco\n",
    "\n",
    "        Convert COCO annotations to YOLO format\n",
    "        >>> convert_coco(\"../datasets/coco/annotations/\", use_segments=True, use_keypoints=False, cls91to80=False)\n",
    "\n",
    "        Convert LVIS annotations to YOLO format\n",
    "        >>> convert_coco(\n",
    "        >>>    \"../datasets/lvis/annotations/\",\n",
    "        ...     use_segments=True,\n",
    "        ...     use_keypoints=False,\n",
    "        ...     cls91to80=False,\n",
    "        ...     lvis=True\n",
    "        ... )\n",
    "\n",
    "    Output:\n",
    "        Generates output files in the specified output directory.\n",
    "    \"\"\"\n",
    "    # Create dataset directory\n",
    "    save_dir = increment_path(save_dir)  # increment if save directory already exists\n",
    "    for p in save_dir / \"labels\", save_dir / \"images\":\n",
    "        p.mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Convert classes\n",
    "    coco80 = coco91_to_coco80_class()\n",
    "\n",
    "    # Import json\n",
    "    for json_file in sorted(Path(labels_dir).resolve().glob(\"*.json\")):\n",
    "        lname = \"\" if lvis else json_file.stem.replace(\"instances_\", \"\")\n",
    "        fn = Path(save_dir) / \"labels\" / lname  # folder name\n",
    "        fn.mkdir(parents=True, exist_ok=True)\n",
    "        if lvis:\n",
    "            # NOTE: create folders for both train and val in advance,\n",
    "            # since LVIS val set contains images from COCO 2017 train in addition to the COCO 2017 val split.\n",
    "            (fn / \"train2017\").mkdir(parents=True, exist_ok=True)\n",
    "            (fn / \"val2017\").mkdir(parents=True, exist_ok=True)\n",
    "        with open(json_file, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Create image dict\n",
    "        images = {f\"{x['id']:d}\": x for x in data[\"images\"]}\n",
    "        # Create image-annotations dict\n",
    "        annotations = defaultdict(list)\n",
    "        for ann in data[\"annotations\"]:\n",
    "            annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "        image_txt = []\n",
    "        # Write labels file\n",
    "        for img_id, anns in TQDM(annotations.items(), desc=f\"Annotations {json_file}\"):\n",
    "            img = images[f\"{img_id:d}\"]\n",
    "            h, w = img[\"height\"], img[\"width\"]\n",
    "            f = str(Path(img[\"coco_url\"]).relative_to(\"http://images.cocodataset.org\")) if lvis else img[\"file_name\"]\n",
    "            if lvis:\n",
    "                image_txt.append(str(Path(\"./images\") / f))\n",
    "\n",
    "            bboxes = []\n",
    "            segments = []\n",
    "            keypoints = []\n",
    "            for ann in anns:\n",
    "                if ann.get(\"iscrowd\", False):\n",
    "                    continue\n",
    "                # The COCO box format is [top left x, top left y, width, height]\n",
    "                box = np.array(ann[\"bbox\"], dtype=np.float64)\n",
    "                box[:2] += box[2:] / 2  # xy top-left corner to center\n",
    "                box[[0, 2]] /= w  # normalize x\n",
    "                box[[1, 3]] /= h  # normalize y\n",
    "                if box[2] <= 0 or box[3] <= 0:  # if w <= 0 and h <= 0\n",
    "                    continue\n",
    "\n",
    "                cls = coco80[ann[\"category_id\"] - 1] if cls91to80 else ann[\"category_id\"]   # class\n",
    "                box = [cls] + box.tolist()\n",
    "                if box not in bboxes:\n",
    "                    bboxes.append(box)\n",
    "                    if use_segments and ann.get(\"segmentation\") is not None:\n",
    "                        if len(ann[\"segmentation\"]) == 0:\n",
    "                            segments.append([])\n",
    "                            continue\n",
    "                        elif len(ann[\"segmentation\"]) > 1:\n",
    "                            s = merge_multi_segment(ann[\"segmentation\"])\n",
    "                            s = (np.concatenate(s, axis=0) / np.array([w, h])).reshape(-1).tolist()\n",
    "                        else:\n",
    "                            s = [j for i in ann[\"segmentation\"] for j in i]  # all segments concatenated\n",
    "                            s = (np.array(s).reshape(-1, 2) / np.array([w, h])).reshape(-1).tolist()\n",
    "                        s = [cls] + s\n",
    "                        segments.append(s)\n",
    "                    if use_keypoints and ann.get(\"keypoints\") is not None:\n",
    "                        keypoints.append(\n",
    "                            box + (np.array(ann[\"keypoints\"]).reshape(-1, 3) / np.array([w, h, 1])).reshape(-1).tolist()\n",
    "                        )\n",
    "\n",
    "            # Write\n",
    "            with open((fn / f).with_suffix(\".txt\"), \"a\", encoding=\"utf-8\") as file:\n",
    "                for i in range(len(bboxes)):\n",
    "                    if use_keypoints:\n",
    "                        line = (*(keypoints[i]),)  # cls, box, keypoints\n",
    "                    else:\n",
    "                        line = (\n",
    "                            *(segments[i] if use_segments and len(segments[i]) > 0 else bboxes[i]),\n",
    "                        )  # cls, box or segments\n",
    "                    file.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n",
    "\n",
    "        if lvis:\n",
    "            filename = Path(save_dir) / json_file.name.replace(\"lvis_v1_\", \"\").replace(\".json\", \".txt\")\n",
    "            with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.writelines(f\"{line}\\n\" for line in image_txt)\n",
    "\n",
    "    LOGGER.info(f\"{'LVIS' if lvis else 'COCO'} data converted successfully.\\nResults saved to {save_dir.resolve()}\")\n",
    "\n",
    "\n",
    "def convert_segment_masks_to_yolo_seg(masks_dir, output_dir, classes):\n",
    "    \"\"\"\n",
    "    Converts a dataset of segmentation mask images to the YOLO segmentation format.\n",
    "\n",
    "    This function takes the directory containing the binary format mask images and converts them into YOLO segmentation format.\n",
    "    The converted masks are saved in the specified output directory.\n",
    "\n",
    "    Args:\n",
    "        masks_dir (str): The path to the directory where all mask images (png, jpg) are stored.\n",
    "        output_dir (str): The path to the directory where the converted YOLO segmentation masks will be stored.\n",
    "        classes (int): Total classes in the dataset i.e. for COCO classes=80\n",
    "\n",
    "    Examples:\n",
    "        >>> from ultralytics.data.converter import convert_segment_masks_to_yolo_seg\n",
    "\n",
    "        The classes here is the total classes in the dataset, for COCO dataset we have 80 classes\n",
    "        >>> convert_segment_masks_to_yolo_seg(\"path/to/masks_directory\", \"path/to/output/directory\", classes=80)\n",
    "\n",
    "    Notes:\n",
    "        The expected directory structure for the masks is:\n",
    "\n",
    "            - masks\n",
    "                ├─ mask_image_01.png or mask_image_01.jpg\n",
    "                ├─ mask_image_02.png or mask_image_02.jpg\n",
    "                ├─ mask_image_03.png or mask_image_03.jpg\n",
    "                └─ mask_image_04.png or mask_image_04.jpg\n",
    "\n",
    "        After execution, the labels will be organized in the following structure:\n",
    "\n",
    "            - output_dir\n",
    "                ├─ mask_yolo_01.txt\n",
    "                ├─ mask_yolo_02.txt\n",
    "                ├─ mask_yolo_03.txt\n",
    "                └─ mask_yolo_04.txt\n",
    "    \"\"\"\n",
    "    pixel_to_class_mapping = {i + 1: i for i in range(classes)}\n",
    "    for mask_path in Path(masks_dir).iterdir():\n",
    "        if mask_path.suffix in {\".png\", \".jpg\"}:\n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)  # Read the mask image in grayscale\n",
    "            img_height, img_width = mask.shape  # Get image dimensions\n",
    "            LOGGER.info(f\"Processing {mask_path} imgsz = {img_height} x {img_width}\")\n",
    "\n",
    "            unique_values = np.unique(mask)  # Get unique pixel values representing different classes\n",
    "            yolo_format_data = []\n",
    "\n",
    "            for value in unique_values:\n",
    "                if value == 0:\n",
    "                    continue  # Skip background\n",
    "                class_index = pixel_to_class_mapping.get(value, -1)\n",
    "                if class_index == -1:\n",
    "                    LOGGER.warning(f\"Unknown class for pixel value {value} in file {mask_path}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Create a binary mask for the current class and find contours\n",
    "                contours, _ = cv2.findContours(\n",
    "                    (mask == value).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "                )  # Find contours\n",
    "\n",
    "                for contour in contours:\n",
    "                    if len(contour) >= 3:  # YOLO requires at least 3 points for a valid segmentation\n",
    "                        contour = contour.squeeze()  # Remove single-dimensional entries\n",
    "                        yolo_format = [class_index]\n",
    "                        for point in contour:\n",
    "                            # Normalize the coordinates\n",
    "                            yolo_format.append(round(point[0] / img_width, 6))  # Rounding to 6 decimal places\n",
    "                            yolo_format.append(round(point[1] / img_height, 6))\n",
    "                        yolo_format_data.append(yolo_format)\n",
    "            # Save Ultralytics YOLO format data to file\n",
    "            output_path = Path(output_dir) / f\"{mask_path.stem}.txt\"\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for item in yolo_format_data:\n",
    "                    line = \" \".join(map(str, item))\n",
    "                    file.write(line + \"\\n\")\n",
    "            LOGGER.info(f\"Processed and stored at {output_path} imgsz = {img_height} x {img_width}\")\n",
    "\n",
    "\n",
    "def convert_dota_to_yolo_obb(dota_root_path: str):\n",
    "    \"\"\"\n",
    "    Converts DOTA dataset annotations to YOLO OBB (Oriented Bounding Box) format.\n",
    "\n",
    "    The function processes images in the 'train' and 'val' folders of the DOTA dataset. For each image, it reads the\n",
    "    associated label from the original labels directory and writes new labels in YOLO OBB format to a new directory.\n",
    "\n",
    "    Args:\n",
    "        dota_root_path (str): The root directory path of the DOTA dataset.\n",
    "\n",
    "    Examples:\n",
    "        >>> from ultralytics.data.converter import convert_dota_to_yolo_obb\n",
    "        >>> convert_dota_to_yolo_obb(\"path/to/DOTA\")\n",
    "\n",
    "    Notes:\n",
    "        The directory structure assumed for the DOTA dataset:\n",
    "\n",
    "            - DOTA\n",
    "                ├─ images\n",
    "                │   ├─ train\n",
    "                │   └─ val\n",
    "                └─ labels\n",
    "                    ├─ train_original\n",
    "                    └─ val_original\n",
    "\n",
    "        After execution, the function will organize the labels into:\n",
    "\n",
    "            - DOTA\n",
    "                └─ labels\n",
    "                    ├─ train\n",
    "                    └─ val\n",
    "    \"\"\"\n",
    "    dota_root_path = Path(dota_root_path)\n",
    "\n",
    "    # Class names to indices mapping\n",
    "    class_mapping = {\n",
    "        \"plane\": 0,\n",
    "        \"ship\": 1,\n",
    "        \"storage-tank\": 2,\n",
    "        \"baseball-diamond\": 3,\n",
    "        \"tennis-court\": 4,\n",
    "        \"basketball-court\": 5,\n",
    "        \"ground-track-field\": 6,\n",
    "        \"harbor\": 7,\n",
    "        \"bridge\": 8,\n",
    "        \"large-vehicle\": 9,\n",
    "        \"small-vehicle\": 10,\n",
    "        \"helicopter\": 11,\n",
    "        \"roundabout\": 12,\n",
    "        \"soccer-ball-field\": 13,\n",
    "        \"swimming-pool\": 14,\n",
    "        \"container-crane\": 15,\n",
    "        \"airport\": 16,\n",
    "        \"helipad\": 17,\n",
    "    }\n",
    "\n",
    "    def convert_label(image_name, image_width, image_height, orig_label_dir, save_dir):\n",
    "        \"\"\"Converts a single image's DOTA annotation to YOLO OBB format and saves it to a specified directory.\"\"\"\n",
    "        orig_label_path = orig_label_dir / f\"{image_name}.txt\"\n",
    "        save_path = save_dir / f\"{image_name}.txt\"\n",
    "\n",
    "        with orig_label_path.open(\"r\") as f, save_path.open(\"w\") as g:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 9:\n",
    "                    continue\n",
    "                class_name = parts[8]\n",
    "                class_idx = class_mapping[class_name]\n",
    "                coords = [float(p) for p in parts[:8]]\n",
    "                normalized_coords = [\n",
    "                    coords[i] / image_width if i % 2 == 0 else coords[i] / image_height for i in range(8)\n",
    "                ]\n",
    "                formatted_coords = [f\"{coord:.6g}\" for coord in normalized_coords]\n",
    "                g.write(f\"{class_idx} {' '.join(formatted_coords)}\\n\")\n",
    "\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        image_dir = dota_root_path / \"images\" / phase\n",
    "        orig_label_dir = dota_root_path / \"labels\" / f\"{phase}_original\"\n",
    "        save_dir = dota_root_path / \"labels\" / phase\n",
    "\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        image_paths = list(image_dir.iterdir())\n",
    "        for image_path in TQDM(image_paths, desc=f\"Processing {phase} images\"):\n",
    "            if image_path.suffix != \".png\":\n",
    "                continue\n",
    "            image_name_without_ext = image_path.stem\n",
    "            img = cv2.imread(str(image_path))\n",
    "            h, w = img.shape[:2]\n",
    "            convert_label(image_name_without_ext, w, h, orig_label_dir, save_dir)\n",
    "\n",
    "\n",
    "def min_index(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Find a pair of indexes with the shortest distance between two arrays of 2D points.\n",
    "\n",
    "    Args:\n",
    "        arr1 (np.ndarray): A NumPy array of shape (N, 2) representing N 2D points.\n",
    "        arr2 (np.ndarray): A NumPy array of shape (M, 2) representing M 2D points.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): A tuple containing the indexes of the points with the shortest distance in arr1 and arr2 respectively.\n",
    "    \"\"\"\n",
    "    dis = ((arr1[:, None, :] - arr2[None, :, :]) ** 2).sum(-1)\n",
    "    return np.unravel_index(np.argmin(dis, axis=None), dis.shape)\n",
    "\n",
    "\n",
    "def merge_multi_segment(segments):\n",
    "    \"\"\"\n",
    "    Merge multiple segments into one list by connecting the coordinates with the minimum distance between each segment.\n",
    "    This function connects these coordinates with a thin line to merge all segments into one.\n",
    "\n",
    "    Args:\n",
    "        segments (List[List]): Original segmentations in COCO's JSON file.\n",
    "                               Each element is a list of coordinates, like [segmentation1, segmentation2,...].\n",
    "\n",
    "    Returns:\n",
    "        s (List[np.ndarray]): A list of connected segments represented as NumPy arrays.\n",
    "    \"\"\"\n",
    "    s = []\n",
    "    segments = [np.array(i).reshape(-1, 2) for i in segments]\n",
    "    idx_list = [[] for _ in range(len(segments))]\n",
    "\n",
    "    # Record the indexes with min distance between each segment\n",
    "    for i in range(1, len(segments)):\n",
    "        idx1, idx2 = min_index(segments[i - 1], segments[i])\n",
    "        idx_list[i - 1].append(idx1)\n",
    "        idx_list[i].append(idx2)\n",
    "\n",
    "    # Use two round to connect all the segments\n",
    "    for k in range(2):\n",
    "        # Forward connection\n",
    "        if k == 0:\n",
    "            for i, idx in enumerate(idx_list):\n",
    "                # Middle segments have two indexes, reverse the index of middle segments\n",
    "                if len(idx) == 2 and idx[0] > idx[1]:\n",
    "                    idx = idx[::-1]\n",
    "                    segments[i] = segments[i][::-1, :]\n",
    "\n",
    "                segments[i] = np.roll(segments[i], -idx[0], axis=0)\n",
    "                segments[i] = np.concatenate([segments[i], segments[i][:1]])\n",
    "                # Deal with the first segment and the last one\n",
    "                if i in {0, len(idx_list) - 1}:\n",
    "                    s.append(segments[i])\n",
    "                else:\n",
    "                    idx = [0, idx[1] - idx[0]]\n",
    "                    s.append(segments[i][idx[0] : idx[1] + 1])\n",
    "\n",
    "        else:\n",
    "            for i in range(len(idx_list) - 1, -1, -1):\n",
    "                if i not in {0, len(idx_list) - 1}:\n",
    "                    idx = idx_list[i]\n",
    "                    nidx = abs(idx[1] - idx[0])\n",
    "                    s.append(segments[i][nidx:])\n",
    "    return s\n",
    "\n",
    "\n",
    "def yolo_bbox2segment(im_dir, save_dir=None, sam_model=\"sam_b.pt\", device=None):\n",
    "    \"\"\"\n",
    "    Converts existing object detection dataset (bounding boxes) to segmentation dataset or oriented bounding box (OBB)\n",
    "    in YOLO format. Generates segmentation data using SAM auto-annotator as needed.\n",
    "\n",
    "    Args:\n",
    "        im_dir (str | Path): Path to image directory to convert.\n",
    "        save_dir (str | Path): Path to save the generated labels, labels will be saved\n",
    "            into `labels-segment` in the same directory level of `im_dir` if save_dir is None.\n",
    "        sam_model (str): Segmentation model to use for intermediate segmentation data.\n",
    "        device (int | str): The specific device to run SAM models.\n",
    "\n",
    "    Notes:\n",
    "        The input directory structure assumed for dataset:\n",
    "\n",
    "            - im_dir\n",
    "                ├─ 001.jpg\n",
    "                ├─ ...\n",
    "                └─ NNN.jpg\n",
    "            - labels\n",
    "                ├─ 001.txt\n",
    "                ├─ ...\n",
    "                └─ NNN.txt\n",
    "    \"\"\"\n",
    "    from ultralytics import SAM\n",
    "    from ultralytics.data import YOLODataset\n",
    "    from ultralytics.utils.ops import xywh2xyxy\n",
    "\n",
    "    # NOTE: add placeholder to pass class index check\n",
    "    dataset = YOLODataset(im_dir, data=dict(names=list(range(1000))))\n",
    "    if len(dataset.labels[0][\"segments\"]) > 0:  # if it's segment data\n",
    "        LOGGER.info(\"Segmentation labels detected, no need to generate new ones!\")\n",
    "        return\n",
    "\n",
    "    LOGGER.info(\"Detection labels detected, generating segment labels by SAM model!\")\n",
    "    sam_model = SAM(sam_model)\n",
    "    for label in TQDM(dataset.labels, total=len(dataset.labels), desc=\"Generating segment labels\"):\n",
    "        h, w = label[\"shape\"]\n",
    "        boxes = label[\"bboxes\"]\n",
    "        if len(boxes) == 0:  # skip empty labels\n",
    "            continue\n",
    "        boxes[:, [0, 2]] *= w\n",
    "        boxes[:, [1, 3]] *= h\n",
    "        im = cv2.imread(label[\"im_file\"])\n",
    "        sam_results = sam_model(im, bboxes=xywh2xyxy(boxes), verbose=False, save=False, device=device)\n",
    "        label[\"segments\"] = sam_results[0].masks.xyn\n",
    "\n",
    "    save_dir = Path(save_dir) if save_dir else Path(im_dir).parent / \"labels-segment\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for label in dataset.labels:\n",
    "        texts = []\n",
    "        lb_name = Path(label[\"im_file\"]).with_suffix(\".txt\").name\n",
    "        txt_file = save_dir / lb_name\n",
    "        cls = label[\"cls\"]\n",
    "        for i, s in enumerate(label[\"segments\"]):\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            line = (int(cls[i]), *s.reshape(-1))\n",
    "            texts.append((\"%g \" * len(line)).rstrip() % line)\n",
    "        with open(txt_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(text + \"\\n\" for text in texts)\n",
    "    LOGGER.info(f\"Generated segment labels saved in {save_dir}\")\n",
    "\n",
    "\n",
    "def create_synthetic_coco_dataset():\n",
    "    \"\"\"\n",
    "    Creates a synthetic COCO dataset with random images based on filenames from label lists.\n",
    "\n",
    "    This function downloads COCO labels, reads image filenames from label list files,\n",
    "    creates synthetic images for train2017 and val2017 subsets, and organizes\n",
    "    them in the COCO dataset structure. It uses multithreading to generate images efficiently.\n",
    "\n",
    "    Examples:\n",
    "        >>> from ultralytics.data.converter import create_synthetic_coco_dataset\n",
    "        >>> create_synthetic_coco_dataset()\n",
    "\n",
    "    Notes:\n",
    "        - Requires internet connection to download label files.\n",
    "        - Generates random RGB images of varying sizes (480x480 to 640x640 pixels).\n",
    "        - Existing test2017 directory is removed as it's not needed.\n",
    "        - Reads image filenames from train2017.txt and val2017.txt files.\n",
    "    \"\"\"\n",
    "\n",
    "    def create_synthetic_image(image_file):\n",
    "        \"\"\"Generates synthetic images with random sizes and colors for dataset augmentation or testing purposes.\"\"\"\n",
    "        if not image_file.exists():\n",
    "            size = (random.randint(480, 640), random.randint(480, 640))\n",
    "            Image.new(\n",
    "                \"RGB\",\n",
    "                size=size,\n",
    "                color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)),\n",
    "            ).save(image_file)\n",
    "\n",
    "    # Download labels\n",
    "    dir = DATASETS_DIR / \"coco\"\n",
    "    url = \"https://github.com/ultralytics/assets/releases/download/v0.0.0/\"\n",
    "    label_zip = \"coco2017labels-segments.zip\"\n",
    "    download([url + label_zip], dir=dir.parent)\n",
    "\n",
    "    # Create synthetic images\n",
    "    shutil.rmtree(dir / \"labels\" / \"test2017\", ignore_errors=True)  # Remove test2017 directory as not needed\n",
    "    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "        for subset in [\"train2017\", \"val2017\"]:\n",
    "            subset_dir = dir / \"images\" / subset\n",
    "            subset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Read image filenames from label list file\n",
    "            label_list_file = dir / f\"{subset}.txt\"\n",
    "            if label_list_file.exists():\n",
    "                with open(label_list_file, encoding=\"utf-8\") as f:\n",
    "                    image_files = [dir / line.strip() for line in f]\n",
    "\n",
    "                # Submit all tasks\n",
    "                futures = [executor.submit(create_synthetic_image, image_file) for image_file in image_files]\n",
    "                for _ in TQDM(as_completed(futures), total=len(futures), desc=f\"Generating images for {subset}\"):\n",
    "                    pass  # The actual work is done in the background\n",
    "            else:\n",
    "                LOGGER.warning(f\"Labels file {label_list_file} does not exist. Skipping image creation for {subset}.\")\n",
    "\n",
    "    LOGGER.info(\"Synthetic COCO dataset created successfully.\")\n",
    "\n",
    "\n",
    "def convert_to_multispectral(path, n_channels=10, replace=False, zip=False):\n",
    "    \"\"\"\n",
    "    Convert RGB images to multispectral images by interpolating across wavelength bands.\n",
    "\n",
    "    This function takes RGB images and interpolates them to create multispectral images with a specified number\n",
    "    of channels. It can process either a single image or a directory of images.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to an image file or directory containing images to convert.\n",
    "        n_channels (int): Number of spectral channels to generate in the output image.\n",
    "        replace (bool): Whether to replace the original image file with the converted one.\n",
    "        zip (bool): Whether to zip the converted images into a zip file.\n",
    "\n",
    "    Examples:\n",
    "        >>> # Convert a single image\n",
    "        >>> convert_to_multispectral(\"path/to/image.jpg\", n_channels=10)\n",
    "        >>> # Convert a dataset\n",
    "        >>> convert_to_multispectral(\"../datasets/coco8\", n_channels=10)\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import interp1d\n",
    "\n",
    "    from ultralytics.data.utils import IMG_FORMATS\n",
    "\n",
    "    path = Path(path)\n",
    "    if path.is_dir():\n",
    "        # Process directory\n",
    "        im_files = sum([list(path.rglob(f\"*.{ext}\")) for ext in (IMG_FORMATS - {\"tif\", \"tiff\"})], [])\n",
    "        for im_path in im_files:\n",
    "            try:\n",
    "                convert_to_multispectral(im_path, n_channels)\n",
    "                if replace:\n",
    "                    im_path.unlink()\n",
    "            except Exception as e:\n",
    "                LOGGER.info(f\"Error converting {im_path}: {e}\")\n",
    "\n",
    "        if zip:\n",
    "            zip_directory(path)\n",
    "    else:\n",
    "        # Process a single image\n",
    "        output_path = path.with_suffix(\".tiff\")\n",
    "        img = cv2.cvtColor(cv2.imread(str(path)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Interpolate all pixels at once\n",
    "        rgb_wavelengths = np.array([650, 510, 475])  # R, G, B wavelengths (nm)\n",
    "        target_wavelengths = np.linspace(450, 700, n_channels)\n",
    "        f = interp1d(rgb_wavelengths.T, img, kind=\"linear\", bounds_error=False, fill_value=\"extrapolate\")\n",
    "        multispectral = f(target_wavelengths)\n",
    "        cv2.imwritemulti(str(output_path), np.clip(multispectral, 0, 255).astype(np.uint8).transpose(2, 0, 1))\n",
    "        LOGGER.info(f\"Converted {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f005ba25-ee09-41d0-8462-1ca5d583d781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotations /home/jupyter/til25-import-torch/cv/json_dataset/annotations.json: 100%|██████████| 19253/19253 [00:02<00:00, 7335.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data converted successfully.\n",
      "Results saved to /home/jupyter/til25-import-torch/cv/yaml_dataset3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For keypoints data (like person_keypoints_val2017.json)\n",
    "convert_coco(\n",
    "    labels_dir=\"/home/jupyter/til25-import-torch/cv/json_dataset\",  # Directory containing your json file\n",
    "    save_dir=\"/home/jupyter/til25-import-torch/cv/yaml_dataset\",\n",
    "    # use_keypoints=False,  # Since you're using keypoints data\n",
    "    use_segments=False,\n",
    "    cls91to80=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec21c6d-5b87-4674-9a61-fbc72d98fffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified split complete: 15402 train, 3851 val\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Paths\n",
    "images_src = Path(\"/home/jupyter/til25-import-torch/cv/json_dataset/images\")\n",
    "labels_src = Path(\"/home/jupyter/til25-import-torch/cv/yaml_dataset/labels/annotations\")\n",
    "dst_base = Path(\"/home/jupyter/til25-import-torch/cv/cv_dataset_split\")\n",
    "\n",
    "# Make sure output folders exist\n",
    "for split in ['train/images', 'train/labels', 'val/images', 'val/labels']:\n",
    "    (dst_base / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Gather image files and determine dominant class for stratification\n",
    "image_paths = list(images_src.glob(\"*.jpg\")) + list(images_src.glob(\"*.png\")) + list(images_src.glob(\"*.jpeg\"))\n",
    "image_names = []\n",
    "class_labels = []\n",
    "\n",
    "for img_path in image_paths:\n",
    "    label_file = labels_src / (img_path.stem + \".txt\")\n",
    "    if not label_file.exists():\n",
    "        continue  # skip if no label\n",
    "\n",
    "    try:\n",
    "        with open(label_file, 'r') as f:\n",
    "            class_ids = [int(line.strip().split()[0]) for line in f.readlines() if line.strip()]\n",
    "        if not class_ids:\n",
    "            continue\n",
    "        dominant_class = Counter(class_ids).most_common(1)[0][0]\n",
    "        image_names.append(img_path.name)\n",
    "        class_labels.append(dominant_class)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {label_file}: {e}\")\n",
    "\n",
    "# Stratified split\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(image_names, class_labels))\n",
    "\n",
    "# Helper to symlink image + label\n",
    "def symlink_subset(indices, split):\n",
    "    for i in indices:\n",
    "        img_name = image_names[i]\n",
    "        label_name = img_name.rsplit('.', 1)[0] + \".txt\"\n",
    "\n",
    "        img_src = images_src / img_name\n",
    "        lbl_src = labels_src / label_name\n",
    "\n",
    "        img_dst = dst_base / split / \"images\" / img_name\n",
    "        lbl_dst = dst_base / split / \"labels\" / label_name\n",
    "\n",
    "        try:\n",
    "            os.symlink(img_src.resolve(), img_dst)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        if lbl_src.exists():\n",
    "            try:\n",
    "                os.symlink(lbl_src.resolve(), lbl_dst)\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "\n",
    "# Create symlinks\n",
    "symlink_subset(train_idx, \"train\")\n",
    "symlink_subset(val_idx, \"val\")\n",
    "\n",
    "print(f\"Stratified split complete: {len(train_idx)} train, {len(val_idx)} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fb645",
   "metadata": {},
   "source": [
    "about 800 images were missing after the split so an analysis function was used to determine the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b07e46c-1985-433d-b405-37d9cca3446d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_coco_annotations(\n",
    "    json_path,\n",
    "    use_segments=False,\n",
    "    use_keypoints=False\n",
    "):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "    annotations_by_image = defaultdict(list)\n",
    "    for ann in data[\"annotations\"]:\n",
    "        annotations_by_image[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    total_images = len(images)\n",
    "    total_annotated_images = 0\n",
    "    skipped_ann_count = 0\n",
    "\n",
    "    print(f\"\\n🔍 Analyzing {json_path}\")\n",
    "    print(f\"Total images in JSON: {total_images}\")\n",
    "    print(\"Logging skipped annotations...\\n\")\n",
    "\n",
    "    for img_id, img in images.items():\n",
    "        anns = annotations_by_image.get(img_id, [])\n",
    "        if not anns:\n",
    "            print(f\"🚫 Image {img['file_name']} has no annotations.\")\n",
    "            continue\n",
    "\n",
    "        valid = False\n",
    "        for ann in anns:\n",
    "            reason = None\n",
    "\n",
    "            if ann.get(\"iscrowd\", 0) == 1:\n",
    "                reason = \"iscrowd=1\"\n",
    "            elif \"bbox\" not in ann or not isinstance(ann[\"bbox\"], list) or len(ann[\"bbox\"]) != 4:\n",
    "                reason = \"Missing or invalid bbox\"\n",
    "            elif ann[\"bbox\"][2] <= 0 or ann[\"bbox\"][3] <= 0:\n",
    "                reason = f\"Invalid bbox size: w={ann['bbox'][2]}, h={ann['bbox'][3]}\"\n",
    "            elif \"category_id\" not in ann:\n",
    "                reason = \"Missing category_id\"\n",
    "            elif use_segments and (\"segmentation\" not in ann or not ann[\"segmentation\"] or not isinstance(ann[\"segmentation\"], list)):\n",
    "                reason = \"Empty or malformed segmentation\"\n",
    "            elif use_keypoints and (\"keypoints\" not in ann or len(ann[\"keypoints\"]) % 3 != 0):\n",
    "                reason = \"Malformed keypoints\"\n",
    "\n",
    "            if reason:\n",
    "                skipped_ann_count += 1\n",
    "                print(f\"❌ Skipping ann ID {ann['id']} in image '{img['file_name']}': {reason}\")\n",
    "            else:\n",
    "                valid = True\n",
    "\n",
    "        if valid:\n",
    "            total_annotated_images += 1\n",
    "\n",
    "    print(\"\\n📊 Summary:\")\n",
    "    print(f\"🖼️  Total images: {total_images}\")\n",
    "    print(f\"✅ Images with at least one valid annotation: {total_annotated_images}\")\n",
    "    print(f\"🚫 Images with no valid annotations: {total_images - total_annotated_images}\")\n",
    "    print(f\"❌ Skipped annotations: {skipped_ann_count}\\n\")\n",
    "\n",
    "# Example usage\n",
    "# analyze_coco_annotations(\"instances_train2017.json\", use_segments=True, use_keypoints=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7543394-96e5-4672-acfd-6e77a80c992c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Analyzing ./json_dataset/annotations.json\n",
      "Total images in JSON: 20000\n",
      "Logging skipped annotations...\n",
      "\n",
      "🚫 Image 17059.jpg has no annotations.\n",
      "🚫 Image 8021.jpg has no annotations.\n",
      "🚫 Image 6320.jpg has no annotations.\n",
      "🚫 Image 5595.jpg has no annotations.\n",
      "🚫 Image 3738.jpg has no annotations.\n",
      "🚫 Image 8104.jpg has no annotations.\n",
      "🚫 Image 10552.jpg has no annotations.\n",
      "🚫 Image 155.jpg has no annotations.\n",
      "🚫 Image 11020.jpg has no annotations.\n",
      "🚫 Image 15269.jpg has no annotations.\n",
      "🚫 Image 7335.jpg has no annotations.\n",
      "🚫 Image 16178.jpg has no annotations.\n",
      "🚫 Image 14970.jpg has no annotations.\n",
      "🚫 Image 9109.jpg has no annotations.\n",
      "🚫 Image 5972.jpg has no annotations.\n",
      "🚫 Image 19698.jpg has no annotations.\n",
      "🚫 Image 8502.jpg has no annotations.\n",
      "🚫 Image 11820.jpg has no annotations.\n",
      "🚫 Image 9176.jpg has no annotations.\n",
      "🚫 Image 11470.jpg has no annotations.\n",
      "🚫 Image 12980.jpg has no annotations.\n",
      "🚫 Image 2719.jpg has no annotations.\n",
      "🚫 Image 7413.jpg has no annotations.\n",
      "🚫 Image 15125.jpg has no annotations.\n",
      "🚫 Image 10290.jpg has no annotations.\n",
      "🚫 Image 19278.jpg has no annotations.\n",
      "🚫 Image 5051.jpg has no annotations.\n",
      "🚫 Image 292.jpg has no annotations.\n",
      "🚫 Image 2103.jpg has no annotations.\n",
      "🚫 Image 7265.jpg has no annotations.\n",
      "🚫 Image 4759.jpg has no annotations.\n",
      "🚫 Image 10545.jpg has no annotations.\n",
      "🚫 Image 3163.jpg has no annotations.\n",
      "🚫 Image 16820.jpg has no annotations.\n",
      "🚫 Image 4558.jpg has no annotations.\n",
      "🚫 Image 8813.jpg has no annotations.\n",
      "🚫 Image 15681.jpg has no annotations.\n",
      "🚫 Image 17012.jpg has no annotations.\n",
      "🚫 Image 6390.jpg has no annotations.\n",
      "🚫 Image 11393.jpg has no annotations.\n",
      "🚫 Image 2361.jpg has no annotations.\n",
      "🚫 Image 16529.jpg has no annotations.\n",
      "🚫 Image 15663.jpg has no annotations.\n",
      "🚫 Image 6441.jpg has no annotations.\n",
      "🚫 Image 10278.jpg has no annotations.\n",
      "🚫 Image 6061.jpg has no annotations.\n",
      "🚫 Image 3832.jpg has no annotations.\n",
      "🚫 Image 19127.jpg has no annotations.\n",
      "🚫 Image 13.jpg has no annotations.\n",
      "🚫 Image 15203.jpg has no annotations.\n",
      "🚫 Image 18602.jpg has no annotations.\n",
      "🚫 Image 17843.jpg has no annotations.\n",
      "🚫 Image 8245.jpg has no annotations.\n",
      "🚫 Image 20.jpg has no annotations.\n",
      "🚫 Image 86.jpg has no annotations.\n",
      "🚫 Image 11745.jpg has no annotations.\n",
      "🚫 Image 2531.jpg has no annotations.\n",
      "🚫 Image 7032.jpg has no annotations.\n",
      "🚫 Image 12498.jpg has no annotations.\n",
      "🚫 Image 12328.jpg has no annotations.\n",
      "🚫 Image 3774.jpg has no annotations.\n",
      "🚫 Image 5272.jpg has no annotations.\n",
      "🚫 Image 10210.jpg has no annotations.\n",
      "🚫 Image 10007.jpg has no annotations.\n",
      "🚫 Image 6370.jpg has no annotations.\n",
      "🚫 Image 18041.jpg has no annotations.\n",
      "🚫 Image 18361.jpg has no annotations.\n",
      "🚫 Image 1052.jpg has no annotations.\n",
      "🚫 Image 8248.jpg has no annotations.\n",
      "🚫 Image 11653.jpg has no annotations.\n",
      "🚫 Image 18952.jpg has no annotations.\n",
      "🚫 Image 4011.jpg has no annotations.\n",
      "🚫 Image 3359.jpg has no annotations.\n",
      "🚫 Image 4730.jpg has no annotations.\n",
      "🚫 Image 3348.jpg has no annotations.\n",
      "🚫 Image 8793.jpg has no annotations.\n",
      "🚫 Image 5331.jpg has no annotations.\n",
      "🚫 Image 18547.jpg has no annotations.\n",
      "🚫 Image 14299.jpg has no annotations.\n",
      "🚫 Image 16712.jpg has no annotations.\n",
      "🚫 Image 10749.jpg has no annotations.\n",
      "🚫 Image 4973.jpg has no annotations.\n",
      "🚫 Image 4393.jpg has no annotations.\n",
      "🚫 Image 14617.jpg has no annotations.\n",
      "🚫 Image 13977.jpg has no annotations.\n",
      "🚫 Image 17754.jpg has no annotations.\n",
      "🚫 Image 4702.jpg has no annotations.\n",
      "🚫 Image 649.jpg has no annotations.\n",
      "🚫 Image 555.jpg has no annotations.\n",
      "🚫 Image 7169.jpg has no annotations.\n",
      "🚫 Image 4242.jpg has no annotations.\n",
      "🚫 Image 14850.jpg has no annotations.\n",
      "🚫 Image 19233.jpg has no annotations.\n",
      "🚫 Image 8615.jpg has no annotations.\n",
      "🚫 Image 8444.jpg has no annotations.\n",
      "🚫 Image 9602.jpg has no annotations.\n",
      "🚫 Image 13223.jpg has no annotations.\n",
      "🚫 Image 9184.jpg has no annotations.\n",
      "🚫 Image 3411.jpg has no annotations.\n",
      "🚫 Image 17360.jpg has no annotations.\n",
      "🚫 Image 13174.jpg has no annotations.\n",
      "🚫 Image 19859.jpg has no annotations.\n",
      "🚫 Image 19641.jpg has no annotations.\n",
      "🚫 Image 3377.jpg has no annotations.\n",
      "🚫 Image 5586.jpg has no annotations.\n",
      "🚫 Image 14115.jpg has no annotations.\n",
      "🚫 Image 12859.jpg has no annotations.\n",
      "🚫 Image 9972.jpg has no annotations.\n",
      "🚫 Image 18954.jpg has no annotations.\n",
      "🚫 Image 13898.jpg has no annotations.\n",
      "🚫 Image 5668.jpg has no annotations.\n",
      "🚫 Image 7775.jpg has no annotations.\n",
      "🚫 Image 10562.jpg has no annotations.\n",
      "🚫 Image 15143.jpg has no annotations.\n",
      "🚫 Image 11557.jpg has no annotations.\n",
      "🚫 Image 7738.jpg has no annotations.\n",
      "🚫 Image 18168.jpg has no annotations.\n",
      "🚫 Image 3675.jpg has no annotations.\n",
      "🚫 Image 6032.jpg has no annotations.\n",
      "🚫 Image 15715.jpg has no annotations.\n",
      "🚫 Image 9982.jpg has no annotations.\n",
      "🚫 Image 11027.jpg has no annotations.\n",
      "🚫 Image 4340.jpg has no annotations.\n",
      "🚫 Image 2126.jpg has no annotations.\n",
      "🚫 Image 17514.jpg has no annotations.\n",
      "🚫 Image 5653.jpg has no annotations.\n",
      "🚫 Image 246.jpg has no annotations.\n",
      "🚫 Image 8018.jpg has no annotations.\n",
      "🚫 Image 17769.jpg has no annotations.\n",
      "🚫 Image 12831.jpg has no annotations.\n",
      "🚫 Image 4322.jpg has no annotations.\n",
      "🚫 Image 6331.jpg has no annotations.\n",
      "🚫 Image 16757.jpg has no annotations.\n",
      "🚫 Image 10700.jpg has no annotations.\n",
      "🚫 Image 10830.jpg has no annotations.\n",
      "🚫 Image 4824.jpg has no annotations.\n",
      "🚫 Image 16594.jpg has no annotations.\n",
      "🚫 Image 2319.jpg has no annotations.\n",
      "🚫 Image 19951.jpg has no annotations.\n",
      "🚫 Image 393.jpg has no annotations.\n",
      "🚫 Image 16760.jpg has no annotations.\n",
      "🚫 Image 15077.jpg has no annotations.\n",
      "🚫 Image 19718.jpg has no annotations.\n",
      "🚫 Image 2440.jpg has no annotations.\n",
      "🚫 Image 12621.jpg has no annotations.\n",
      "🚫 Image 19862.jpg has no annotations.\n",
      "🚫 Image 9924.jpg has no annotations.\n",
      "🚫 Image 5390.jpg has no annotations.\n",
      "🚫 Image 6049.jpg has no annotations.\n",
      "🚫 Image 15590.jpg has no annotations.\n",
      "🚫 Image 17499.jpg has no annotations.\n",
      "🚫 Image 10316.jpg has no annotations.\n",
      "🚫 Image 3106.jpg has no annotations.\n",
      "🚫 Image 8428.jpg has no annotations.\n",
      "🚫 Image 19709.jpg has no annotations.\n",
      "🚫 Image 17945.jpg has no annotations.\n",
      "🚫 Image 10966.jpg has no annotations.\n",
      "🚫 Image 3560.jpg has no annotations.\n",
      "🚫 Image 8782.jpg has no annotations.\n",
      "🚫 Image 15166.jpg has no annotations.\n",
      "🚫 Image 14205.jpg has no annotations.\n",
      "🚫 Image 17036.jpg has no annotations.\n",
      "🚫 Image 7895.jpg has no annotations.\n",
      "🚫 Image 3375.jpg has no annotations.\n",
      "🚫 Image 3531.jpg has no annotations.\n",
      "🚫 Image 6578.jpg has no annotations.\n",
      "🚫 Image 1295.jpg has no annotations.\n",
      "🚫 Image 4065.jpg has no annotations.\n",
      "🚫 Image 14851.jpg has no annotations.\n",
      "🚫 Image 12286.jpg has no annotations.\n",
      "🚫 Image 19399.jpg has no annotations.\n",
      "🚫 Image 12412.jpg has no annotations.\n",
      "🚫 Image 5608.jpg has no annotations.\n",
      "🚫 Image 5829.jpg has no annotations.\n",
      "🚫 Image 3049.jpg has no annotations.\n",
      "🚫 Image 17755.jpg has no annotations.\n",
      "🚫 Image 1095.jpg has no annotations.\n",
      "🚫 Image 4010.jpg has no annotations.\n",
      "🚫 Image 2786.jpg has no annotations.\n",
      "🚫 Image 14145.jpg has no annotations.\n",
      "🚫 Image 11917.jpg has no annotations.\n",
      "🚫 Image 17836.jpg has no annotations.\n",
      "🚫 Image 15071.jpg has no annotations.\n",
      "🚫 Image 7671.jpg has no annotations.\n",
      "🚫 Image 4768.jpg has no annotations.\n",
      "🚫 Image 6472.jpg has no annotations.\n",
      "🚫 Image 387.jpg has no annotations.\n",
      "🚫 Image 19770.jpg has no annotations.\n",
      "🚫 Image 13196.jpg has no annotations.\n",
      "🚫 Image 12090.jpg has no annotations.\n",
      "🚫 Image 18500.jpg has no annotations.\n",
      "🚫 Image 8397.jpg has no annotations.\n",
      "🚫 Image 11606.jpg has no annotations.\n",
      "🚫 Image 8761.jpg has no annotations.\n",
      "🚫 Image 1627.jpg has no annotations.\n",
      "🚫 Image 13082.jpg has no annotations.\n",
      "🚫 Image 15644.jpg has no annotations.\n",
      "🚫 Image 7315.jpg has no annotations.\n",
      "🚫 Image 15821.jpg has no annotations.\n",
      "🚫 Image 13043.jpg has no annotations.\n",
      "🚫 Image 14969.jpg has no annotations.\n",
      "🚫 Image 17427.jpg has no annotations.\n",
      "🚫 Image 18390.jpg has no annotations.\n",
      "🚫 Image 1332.jpg has no annotations.\n",
      "🚫 Image 10069.jpg has no annotations.\n",
      "🚫 Image 270.jpg has no annotations.\n",
      "🚫 Image 16382.jpg has no annotations.\n",
      "🚫 Image 15410.jpg has no annotations.\n",
      "🚫 Image 5073.jpg has no annotations.\n",
      "🚫 Image 12874.jpg has no annotations.\n",
      "🚫 Image 19300.jpg has no annotations.\n",
      "🚫 Image 5923.jpg has no annotations.\n",
      "🚫 Image 7105.jpg has no annotations.\n",
      "🚫 Image 9420.jpg has no annotations.\n",
      "🚫 Image 8942.jpg has no annotations.\n",
      "🚫 Image 11116.jpg has no annotations.\n",
      "🚫 Image 238.jpg has no annotations.\n",
      "🚫 Image 10696.jpg has no annotations.\n",
      "🚫 Image 14586.jpg has no annotations.\n",
      "🚫 Image 16040.jpg has no annotations.\n",
      "🚫 Image 8822.jpg has no annotations.\n",
      "🚫 Image 1046.jpg has no annotations.\n",
      "🚫 Image 7680.jpg has no annotations.\n",
      "🚫 Image 19212.jpg has no annotations.\n",
      "🚫 Image 17538.jpg has no annotations.\n",
      "🚫 Image 2190.jpg has no annotations.\n",
      "🚫 Image 647.jpg has no annotations.\n",
      "🚫 Image 19609.jpg has no annotations.\n",
      "🚫 Image 10927.jpg has no annotations.\n",
      "🚫 Image 9889.jpg has no annotations.\n",
      "🚫 Image 14922.jpg has no annotations.\n",
      "🚫 Image 18344.jpg has no annotations.\n",
      "🚫 Image 17759.jpg has no annotations.\n",
      "🚫 Image 7000.jpg has no annotations.\n",
      "🚫 Image 11241.jpg has no annotations.\n",
      "🚫 Image 6397.jpg has no annotations.\n",
      "🚫 Image 13928.jpg has no annotations.\n",
      "🚫 Image 16972.jpg has no annotations.\n",
      "🚫 Image 11579.jpg has no annotations.\n",
      "🚫 Image 9411.jpg has no annotations.\n",
      "🚫 Image 19134.jpg has no annotations.\n",
      "🚫 Image 8491.jpg has no annotations.\n",
      "🚫 Image 8540.jpg has no annotations.\n",
      "🚫 Image 15210.jpg has no annotations.\n",
      "🚫 Image 4639.jpg has no annotations.\n",
      "🚫 Image 9289.jpg has no annotations.\n",
      "🚫 Image 17034.jpg has no annotations.\n",
      "🚫 Image 3930.jpg has no annotations.\n",
      "🚫 Image 10723.jpg has no annotations.\n",
      "🚫 Image 3881.jpg has no annotations.\n",
      "🚫 Image 17746.jpg has no annotations.\n",
      "🚫 Image 17015.jpg has no annotations.\n",
      "🚫 Image 7573.jpg has no annotations.\n",
      "🚫 Image 11705.jpg has no annotations.\n",
      "🚫 Image 8983.jpg has no annotations.\n",
      "🚫 Image 6142.jpg has no annotations.\n",
      "🚫 Image 13194.jpg has no annotations.\n",
      "🚫 Image 13112.jpg has no annotations.\n",
      "🚫 Image 17030.jpg has no annotations.\n",
      "🚫 Image 18768.jpg has no annotations.\n",
      "🚫 Image 716.jpg has no annotations.\n",
      "🚫 Image 5140.jpg has no annotations.\n",
      "🚫 Image 3078.jpg has no annotations.\n",
      "🚫 Image 10153.jpg has no annotations.\n",
      "🚫 Image 565.jpg has no annotations.\n",
      "🚫 Image 19534.jpg has no annotations.\n",
      "🚫 Image 2878.jpg has no annotations.\n",
      "🚫 Image 2415.jpg has no annotations.\n",
      "🚫 Image 17424.jpg has no annotations.\n",
      "🚫 Image 11137.jpg has no annotations.\n",
      "🚫 Image 9354.jpg has no annotations.\n",
      "🚫 Image 15667.jpg has no annotations.\n",
      "🚫 Image 17844.jpg has no annotations.\n",
      "🚫 Image 3795.jpg has no annotations.\n",
      "🚫 Image 4161.jpg has no annotations.\n",
      "🚫 Image 17596.jpg has no annotations.\n",
      "🚫 Image 18394.jpg has no annotations.\n",
      "🚫 Image 8165.jpg has no annotations.\n",
      "🚫 Image 9515.jpg has no annotations.\n",
      "🚫 Image 18239.jpg has no annotations.\n",
      "🚫 Image 4091.jpg has no annotations.\n",
      "🚫 Image 17671.jpg has no annotations.\n",
      "🚫 Image 9272.jpg has no annotations.\n",
      "🚫 Image 10923.jpg has no annotations.\n",
      "🚫 Image 18420.jpg has no annotations.\n",
      "🚫 Image 17512.jpg has no annotations.\n",
      "🚫 Image 13370.jpg has no annotations.\n",
      "🚫 Image 12041.jpg has no annotations.\n",
      "🚫 Image 8422.jpg has no annotations.\n",
      "🚫 Image 4628.jpg has no annotations.\n",
      "🚫 Image 10595.jpg has no annotations.\n",
      "🚫 Image 15749.jpg has no annotations.\n",
      "🚫 Image 13180.jpg has no annotations.\n",
      "🚫 Image 19015.jpg has no annotations.\n",
      "🚫 Image 4308.jpg has no annotations.\n",
      "🚫 Image 3203.jpg has no annotations.\n",
      "🚫 Image 9341.jpg has no annotations.\n",
      "🚫 Image 250.jpg has no annotations.\n",
      "🚫 Image 13687.jpg has no annotations.\n",
      "🚫 Image 19945.jpg has no annotations.\n",
      "🚫 Image 14366.jpg has no annotations.\n",
      "🚫 Image 2485.jpg has no annotations.\n",
      "🚫 Image 12627.jpg has no annotations.\n",
      "🚫 Image 6870.jpg has no annotations.\n",
      "🚫 Image 12098.jpg has no annotations.\n",
      "🚫 Image 7449.jpg has no annotations.\n",
      "🚫 Image 5344.jpg has no annotations.\n",
      "🚫 Image 8877.jpg has no annotations.\n",
      "🚫 Image 13475.jpg has no annotations.\n",
      "🚫 Image 18904.jpg has no annotations.\n",
      "🚫 Image 10044.jpg has no annotations.\n",
      "🚫 Image 19972.jpg has no annotations.\n",
      "🚫 Image 14519.jpg has no annotations.\n",
      "🚫 Image 17299.jpg has no annotations.\n",
      "🚫 Image 3051.jpg has no annotations.\n",
      "🚫 Image 15282.jpg has no annotations.\n",
      "🚫 Image 2422.jpg has no annotations.\n",
      "🚫 Image 4354.jpg has no annotations.\n",
      "🚫 Image 19130.jpg has no annotations.\n",
      "🚫 Image 2328.jpg has no annotations.\n",
      "🚫 Image 13798.jpg has no annotations.\n",
      "🚫 Image 9754.jpg has no annotations.\n",
      "🚫 Image 11022.jpg has no annotations.\n",
      "🚫 Image 17784.jpg has no annotations.\n",
      "🚫 Image 17179.jpg has no annotations.\n",
      "🚫 Image 3326.jpg has no annotations.\n",
      "🚫 Image 11995.jpg has no annotations.\n",
      "🚫 Image 3137.jpg has no annotations.\n",
      "🚫 Image 16725.jpg has no annotations.\n",
      "🚫 Image 10461.jpg has no annotations.\n",
      "🚫 Image 11986.jpg has no annotations.\n",
      "🚫 Image 9371.jpg has no annotations.\n",
      "🚫 Image 7987.jpg has no annotations.\n",
      "🚫 Image 7504.jpg has no annotations.\n",
      "🚫 Image 2085.jpg has no annotations.\n",
      "🚫 Image 15521.jpg has no annotations.\n",
      "🚫 Image 14555.jpg has no annotations.\n",
      "🚫 Image 6739.jpg has no annotations.\n",
      "🚫 Image 14780.jpg has no annotations.\n",
      "🚫 Image 881.jpg has no annotations.\n",
      "🚫 Image 4618.jpg has no annotations.\n",
      "🚫 Image 17955.jpg has no annotations.\n",
      "🚫 Image 2413.jpg has no annotations.\n",
      "🚫 Image 5186.jpg has no annotations.\n",
      "🚫 Image 11484.jpg has no annotations.\n",
      "🚫 Image 15526.jpg has no annotations.\n",
      "🚫 Image 12681.jpg has no annotations.\n",
      "🚫 Image 12751.jpg has no annotations.\n",
      "🚫 Image 16171.jpg has no annotations.\n",
      "🚫 Image 5178.jpg has no annotations.\n",
      "🚫 Image 19147.jpg has no annotations.\n",
      "🚫 Image 9318.jpg has no annotations.\n",
      "🚫 Image 9061.jpg has no annotations.\n",
      "🚫 Image 6019.jpg has no annotations.\n",
      "🚫 Image 11030.jpg has no annotations.\n",
      "🚫 Image 18222.jpg has no annotations.\n",
      "🚫 Image 19580.jpg has no annotations.\n",
      "🚫 Image 9967.jpg has no annotations.\n",
      "🚫 Image 4929.jpg has no annotations.\n",
      "🚫 Image 7710.jpg has no annotations.\n",
      "🚫 Image 19478.jpg has no annotations.\n",
      "🚫 Image 18282.jpg has no annotations.\n",
      "🚫 Image 3291.jpg has no annotations.\n",
      "🚫 Image 13703.jpg has no annotations.\n",
      "🚫 Image 8381.jpg has no annotations.\n",
      "🚫 Image 1227.jpg has no annotations.\n",
      "🚫 Image 6288.jpg has no annotations.\n",
      "🚫 Image 8488.jpg has no annotations.\n",
      "🚫 Image 6513.jpg has no annotations.\n",
      "🚫 Image 17629.jpg has no annotations.\n",
      "🚫 Image 848.jpg has no annotations.\n",
      "🚫 Image 1539.jpg has no annotations.\n",
      "🚫 Image 3139.jpg has no annotations.\n",
      "🚫 Image 12516.jpg has no annotations.\n",
      "🚫 Image 1888.jpg has no annotations.\n",
      "🚫 Image 14368.jpg has no annotations.\n",
      "🚫 Image 5507.jpg has no annotations.\n",
      "🚫 Image 9901.jpg has no annotations.\n",
      "🚫 Image 8535.jpg has no annotations.\n",
      "🚫 Image 6955.jpg has no annotations.\n",
      "🚫 Image 16498.jpg has no annotations.\n",
      "🚫 Image 11004.jpg has no annotations.\n",
      "🚫 Image 4939.jpg has no annotations.\n",
      "🚫 Image 6271.jpg has no annotations.\n",
      "🚫 Image 14734.jpg has no annotations.\n",
      "🚫 Image 2363.jpg has no annotations.\n",
      "🚫 Image 7166.jpg has no annotations.\n",
      "🚫 Image 18295.jpg has no annotations.\n",
      "🚫 Image 3480.jpg has no annotations.\n",
      "🚫 Image 18718.jpg has no annotations.\n",
      "🚫 Image 12931.jpg has no annotations.\n",
      "🚫 Image 8768.jpg has no annotations.\n",
      "🚫 Image 9671.jpg has no annotations.\n",
      "🚫 Image 12993.jpg has no annotations.\n",
      "🚫 Image 3524.jpg has no annotations.\n",
      "🚫 Image 5026.jpg has no annotations.\n",
      "🚫 Image 1097.jpg has no annotations.\n",
      "🚫 Image 9204.jpg has no annotations.\n",
      "🚫 Image 19994.jpg has no annotations.\n",
      "🚫 Image 12294.jpg has no annotations.\n",
      "🚫 Image 6454.jpg has no annotations.\n",
      "🚫 Image 1518.jpg has no annotations.\n",
      "🚫 Image 14772.jpg has no annotations.\n",
      "🚫 Image 9676.jpg has no annotations.\n",
      "🚫 Image 10726.jpg has no annotations.\n",
      "🚫 Image 4849.jpg has no annotations.\n",
      "🚫 Image 12307.jpg has no annotations.\n",
      "🚫 Image 18051.jpg has no annotations.\n",
      "🚫 Image 11364.jpg has no annotations.\n",
      "🚫 Image 1065.jpg has no annotations.\n",
      "🚫 Image 12943.jpg has no annotations.\n",
      "🚫 Image 6489.jpg has no annotations.\n",
      "🚫 Image 1690.jpg has no annotations.\n",
      "🚫 Image 162.jpg has no annotations.\n",
      "🚫 Image 7071.jpg has no annotations.\n",
      "🚫 Image 3974.jpg has no annotations.\n",
      "🚫 Image 235.jpg has no annotations.\n",
      "🚫 Image 2199.jpg has no annotations.\n",
      "🚫 Image 17688.jpg has no annotations.\n",
      "🚫 Image 17721.jpg has no annotations.\n",
      "🚫 Image 15814.jpg has no annotations.\n",
      "🚫 Image 6241.jpg has no annotations.\n",
      "🚫 Image 16326.jpg has no annotations.\n",
      "🚫 Image 7238.jpg has no annotations.\n",
      "🚫 Image 9919.jpg has no annotations.\n",
      "🚫 Image 1206.jpg has no annotations.\n",
      "🚫 Image 12936.jpg has no annotations.\n",
      "🚫 Image 16571.jpg has no annotations.\n",
      "🚫 Image 2091.jpg has no annotations.\n",
      "🚫 Image 5740.jpg has no annotations.\n",
      "🚫 Image 14364.jpg has no annotations.\n",
      "🚫 Image 8593.jpg has no annotations.\n",
      "🚫 Image 12574.jpg has no annotations.\n",
      "🚫 Image 10451.jpg has no annotations.\n",
      "🚫 Image 2187.jpg has no annotations.\n",
      "🚫 Image 10060.jpg has no annotations.\n",
      "🚫 Image 10245.jpg has no annotations.\n",
      "🚫 Image 15683.jpg has no annotations.\n",
      "🚫 Image 19101.jpg has no annotations.\n",
      "🚫 Image 19864.jpg has no annotations.\n",
      "🚫 Image 5008.jpg has no annotations.\n",
      "🚫 Image 5343.jpg has no annotations.\n",
      "🚫 Image 5252.jpg has no annotations.\n",
      "🚫 Image 1901.jpg has no annotations.\n",
      "🚫 Image 16343.jpg has no annotations.\n",
      "🚫 Image 9027.jpg has no annotations.\n",
      "🚫 Image 17778.jpg has no annotations.\n",
      "🚫 Image 18426.jpg has no annotations.\n",
      "🚫 Image 9664.jpg has no annotations.\n",
      "🚫 Image 816.jpg has no annotations.\n",
      "🚫 Image 15753.jpg has no annotations.\n",
      "🚫 Image 1260.jpg has no annotations.\n",
      "🚫 Image 16444.jpg has no annotations.\n",
      "🚫 Image 322.jpg has no annotations.\n",
      "🚫 Image 13186.jpg has no annotations.\n",
      "🚫 Image 1138.jpg has no annotations.\n",
      "🚫 Image 10030.jpg has no annotations.\n",
      "🚫 Image 3602.jpg has no annotations.\n",
      "🚫 Image 3190.jpg has no annotations.\n",
      "🚫 Image 3120.jpg has no annotations.\n",
      "🚫 Image 1511.jpg has no annotations.\n",
      "🚫 Image 2228.jpg has no annotations.\n",
      "🚫 Image 3115.jpg has no annotations.\n",
      "🚫 Image 2389.jpg has no annotations.\n",
      "🚫 Image 9682.jpg has no annotations.\n",
      "🚫 Image 7977.jpg has no annotations.\n",
      "🚫 Image 12595.jpg has no annotations.\n",
      "🚫 Image 11960.jpg has no annotations.\n",
      "🚫 Image 6118.jpg has no annotations.\n",
      "🚫 Image 3522.jpg has no annotations.\n",
      "🚫 Image 13489.jpg has no annotations.\n",
      "🚫 Image 14958.jpg has no annotations.\n",
      "🚫 Image 2871.jpg has no annotations.\n",
      "🚫 Image 15059.jpg has no annotations.\n",
      "🚫 Image 6004.jpg has no annotations.\n",
      "🚫 Image 14300.jpg has no annotations.\n",
      "🚫 Image 17451.jpg has no annotations.\n",
      "🚫 Image 6728.jpg has no annotations.\n",
      "🚫 Image 16774.jpg has no annotations.\n",
      "🚫 Image 14732.jpg has no annotations.\n",
      "🚫 Image 3085.jpg has no annotations.\n",
      "🚫 Image 9094.jpg has no annotations.\n",
      "🚫 Image 13419.jpg has no annotations.\n",
      "🚫 Image 1505.jpg has no annotations.\n",
      "🚫 Image 94.jpg has no annotations.\n",
      "🚫 Image 178.jpg has no annotations.\n",
      "🚫 Image 8870.jpg has no annotations.\n",
      "🚫 Image 12340.jpg has no annotations.\n",
      "🚫 Image 15412.jpg has no annotations.\n",
      "🚫 Image 2129.jpg has no annotations.\n",
      "🚫 Image 7396.jpg has no annotations.\n",
      "🚫 Image 8002.jpg has no annotations.\n",
      "🚫 Image 17235.jpg has no annotations.\n",
      "🚫 Image 6536.jpg has no annotations.\n",
      "🚫 Image 9500.jpg has no annotations.\n",
      "🚫 Image 11330.jpg has no annotations.\n",
      "🚫 Image 18157.jpg has no annotations.\n",
      "🚫 Image 866.jpg has no annotations.\n",
      "🚫 Image 1778.jpg has no annotations.\n",
      "🚫 Image 5208.jpg has no annotations.\n",
      "🚫 Image 16238.jpg has no annotations.\n",
      "🚫 Image 14845.jpg has no annotations.\n",
      "🚫 Image 9575.jpg has no annotations.\n",
      "🚫 Image 13386.jpg has no annotations.\n",
      "🚫 Image 1214.jpg has no annotations.\n",
      "🚫 Image 15968.jpg has no annotations.\n",
      "🚫 Image 1354.jpg has no annotations.\n",
      "🚫 Image 13050.jpg has no annotations.\n",
      "🚫 Image 16589.jpg has no annotations.\n",
      "🚫 Image 15249.jpg has no annotations.\n",
      "🚫 Image 6214.jpg has no annotations.\n",
      "🚫 Image 13907.jpg has no annotations.\n",
      "🚫 Image 4591.jpg has no annotations.\n",
      "🚫 Image 7845.jpg has no annotations.\n",
      "🚫 Image 16445.jpg has no annotations.\n",
      "🚫 Image 7255.jpg has no annotations.\n",
      "🚫 Image 16788.jpg has no annotations.\n",
      "🚫 Image 6866.jpg has no annotations.\n",
      "🚫 Image 7201.jpg has no annotations.\n",
      "🚫 Image 5427.jpg has no annotations.\n",
      "🚫 Image 19041.jpg has no annotations.\n",
      "🚫 Image 13161.jpg has no annotations.\n",
      "🚫 Image 7046.jpg has no annotations.\n",
      "🚫 Image 4912.jpg has no annotations.\n",
      "🚫 Image 7421.jpg has no annotations.\n",
      "🚫 Image 19219.jpg has no annotations.\n",
      "🚫 Image 2736.jpg has no annotations.\n",
      "🚫 Image 15723.jpg has no annotations.\n",
      "🚫 Image 3808.jpg has no annotations.\n",
      "🚫 Image 8757.jpg has no annotations.\n",
      "🚫 Image 14933.jpg has no annotations.\n",
      "🚫 Image 5557.jpg has no annotations.\n",
      "🚫 Image 2856.jpg has no annotations.\n",
      "🚫 Image 14649.jpg has no annotations.\n",
      "🚫 Image 14024.jpg has no annotations.\n",
      "🚫 Image 1608.jpg has no annotations.\n",
      "🚫 Image 12426.jpg has no annotations.\n",
      "🚫 Image 10018.jpg has no annotations.\n",
      "🚫 Image 17148.jpg has no annotations.\n",
      "🚫 Image 12850.jpg has no annotations.\n",
      "🚫 Image 2048.jpg has no annotations.\n",
      "🚫 Image 18525.jpg has no annotations.\n",
      "🚫 Image 2133.jpg has no annotations.\n",
      "🚫 Image 18223.jpg has no annotations.\n",
      "🚫 Image 1730.jpg has no annotations.\n",
      "🚫 Image 12271.jpg has no annotations.\n",
      "🚫 Image 11145.jpg has no annotations.\n",
      "🚫 Image 2933.jpg has no annotations.\n",
      "🚫 Image 11818.jpg has no annotations.\n",
      "🚫 Image 19519.jpg has no annotations.\n",
      "🚫 Image 6098.jpg has no annotations.\n",
      "🚫 Image 10047.jpg has no annotations.\n",
      "🚫 Image 19472.jpg has no annotations.\n",
      "🚫 Image 11438.jpg has no annotations.\n",
      "🚫 Image 15162.jpg has no annotations.\n",
      "🚫 Image 16338.jpg has no annotations.\n",
      "🚫 Image 14816.jpg has no annotations.\n",
      "🚫 Image 14122.jpg has no annotations.\n",
      "🚫 Image 14952.jpg has no annotations.\n",
      "🚫 Image 8132.jpg has no annotations.\n",
      "🚫 Image 8146.jpg has no annotations.\n",
      "🚫 Image 13139.jpg has no annotations.\n",
      "🚫 Image 15488.jpg has no annotations.\n",
      "🚫 Image 7698.jpg has no annotations.\n",
      "🚫 Image 6111.jpg has no annotations.\n",
      "🚫 Image 15985.jpg has no annotations.\n",
      "🚫 Image 15216.jpg has no annotations.\n",
      "🚫 Image 263.jpg has no annotations.\n",
      "🚫 Image 139.jpg has no annotations.\n",
      "🚫 Image 10859.jpg has no annotations.\n",
      "🚫 Image 19777.jpg has no annotations.\n",
      "🚫 Image 19501.jpg has no annotations.\n",
      "🚫 Image 4126.jpg has no annotations.\n",
      "🚫 Image 13716.jpg has no annotations.\n",
      "🚫 Image 10953.jpg has no annotations.\n",
      "🚫 Image 7998.jpg has no annotations.\n",
      "🚫 Image 19815.jpg has no annotations.\n",
      "🚫 Image 9085.jpg has no annotations.\n",
      "🚫 Image 1912.jpg has no annotations.\n",
      "🚫 Image 13914.jpg has no annotations.\n",
      "🚫 Image 5248.jpg has no annotations.\n",
      "🚫 Image 15817.jpg has no annotations.\n",
      "🚫 Image 6013.jpg has no annotations.\n",
      "🚫 Image 19647.jpg has no annotations.\n",
      "🚫 Image 6858.jpg has no annotations.\n",
      "🚫 Image 7794.jpg has no annotations.\n",
      "🚫 Image 1861.jpg has no annotations.\n",
      "🚫 Image 19442.jpg has no annotations.\n",
      "🚫 Image 12218.jpg has no annotations.\n",
      "🚫 Image 7022.jpg has no annotations.\n",
      "🚫 Image 1790.jpg has no annotations.\n",
      "🚫 Image 18507.jpg has no annotations.\n",
      "🚫 Image 4601.jpg has no annotations.\n",
      "🚫 Image 12054.jpg has no annotations.\n",
      "🚫 Image 12706.jpg has no annotations.\n",
      "🚫 Image 1050.jpg has no annotations.\n",
      "🚫 Image 2902.jpg has no annotations.\n",
      "🚫 Image 2335.jpg has no annotations.\n",
      "🚫 Image 13675.jpg has no annotations.\n",
      "🚫 Image 4729.jpg has no annotations.\n",
      "🚫 Image 12716.jpg has no annotations.\n",
      "🚫 Image 16624.jpg has no annotations.\n",
      "🚫 Image 2876.jpg has no annotations.\n",
      "🚫 Image 12380.jpg has no annotations.\n",
      "🚫 Image 8487.jpg has no annotations.\n",
      "🚫 Image 16132.jpg has no annotations.\n",
      "🚫 Image 17282.jpg has no annotations.\n",
      "🚫 Image 12204.jpg has no annotations.\n",
      "🚫 Image 6072.jpg has no annotations.\n",
      "🚫 Image 9412.jpg has no annotations.\n",
      "🚫 Image 6943.jpg has no annotations.\n",
      "🚫 Image 5579.jpg has no annotations.\n",
      "🚫 Image 13406.jpg has no annotations.\n",
      "🚫 Image 396.jpg has no annotations.\n",
      "🚫 Image 17392.jpg has no annotations.\n",
      "🚫 Image 12535.jpg has no annotations.\n",
      "🚫 Image 4099.jpg has no annotations.\n",
      "🚫 Image 10593.jpg has no annotations.\n",
      "🚫 Image 19952.jpg has no annotations.\n",
      "🚫 Image 422.jpg has no annotations.\n",
      "🚫 Image 11638.jpg has no annotations.\n",
      "🚫 Image 10067.jpg has no annotations.\n",
      "🚫 Image 3477.jpg has no annotations.\n",
      "🚫 Image 9601.jpg has no annotations.\n",
      "🚫 Image 10834.jpg has no annotations.\n",
      "🚫 Image 9687.jpg has no annotations.\n",
      "🚫 Image 8446.jpg has no annotations.\n",
      "🚫 Image 5282.jpg has no annotations.\n",
      "🚫 Image 4986.jpg has no annotations.\n",
      "🚫 Image 3660.jpg has no annotations.\n",
      "🚫 Image 17254.jpg has no annotations.\n",
      "🚫 Image 18666.jpg has no annotations.\n",
      "🚫 Image 5838.jpg has no annotations.\n",
      "🚫 Image 13336.jpg has no annotations.\n",
      "🚫 Image 13498.jpg has no annotations.\n",
      "🚫 Image 1866.jpg has no annotations.\n",
      "🚫 Image 5263.jpg has no annotations.\n",
      "🚫 Image 18842.jpg has no annotations.\n",
      "🚫 Image 16398.jpg has no annotations.\n",
      "🚫 Image 3519.jpg has no annotations.\n",
      "🚫 Image 13955.jpg has no annotations.\n",
      "🚫 Image 9583.jpg has no annotations.\n",
      "🚫 Image 18519.jpg has no annotations.\n",
      "🚫 Image 16818.jpg has no annotations.\n",
      "🚫 Image 5836.jpg has no annotations.\n",
      "🚫 Image 2216.jpg has no annotations.\n",
      "🚫 Image 3898.jpg has no annotations.\n",
      "🚫 Image 17089.jpg has no annotations.\n",
      "🚫 Image 15342.jpg has no annotations.\n",
      "🚫 Image 11698.jpg has no annotations.\n",
      "🚫 Image 18621.jpg has no annotations.\n",
      "🚫 Image 18501.jpg has no annotations.\n",
      "🚫 Image 19225.jpg has no annotations.\n",
      "🚫 Image 16199.jpg has no annotations.\n",
      "🚫 Image 13213.jpg has no annotations.\n",
      "🚫 Image 12771.jpg has no annotations.\n",
      "🚫 Image 10491.jpg has no annotations.\n",
      "🚫 Image 7850.jpg has no annotations.\n",
      "🚫 Image 19400.jpg has no annotations.\n",
      "🚫 Image 14796.jpg has no annotations.\n",
      "🚫 Image 13649.jpg has no annotations.\n",
      "🚫 Image 17602.jpg has no annotations.\n",
      "🚫 Image 8185.jpg has no annotations.\n",
      "🚫 Image 16564.jpg has no annotations.\n",
      "🚫 Image 3927.jpg has no annotations.\n",
      "🚫 Image 17652.jpg has no annotations.\n",
      "🚫 Image 16834.jpg has no annotations.\n",
      "🚫 Image 53.jpg has no annotations.\n",
      "🚫 Image 16237.jpg has no annotations.\n",
      "🚫 Image 6734.jpg has no annotations.\n",
      "🚫 Image 1006.jpg has no annotations.\n",
      "🚫 Image 17255.jpg has no annotations.\n",
      "🚫 Image 11813.jpg has no annotations.\n",
      "🚫 Image 10699.jpg has no annotations.\n",
      "🚫 Image 10802.jpg has no annotations.\n",
      "🚫 Image 18199.jpg has no annotations.\n",
      "🚫 Image 9021.jpg has no annotations.\n",
      "🚫 Image 8437.jpg has no annotations.\n",
      "🚫 Image 10397.jpg has no annotations.\n",
      "🚫 Image 7627.jpg has no annotations.\n",
      "🚫 Image 1825.jpg has no annotations.\n",
      "🚫 Image 18810.jpg has no annotations.\n",
      "🚫 Image 2488.jpg has no annotations.\n",
      "🚫 Image 5167.jpg has no annotations.\n",
      "🚫 Image 7617.jpg has no annotations.\n",
      "🚫 Image 17572.jpg has no annotations.\n",
      "🚫 Image 7913.jpg has no annotations.\n",
      "🚫 Image 15111.jpg has no annotations.\n",
      "🚫 Image 4206.jpg has no annotations.\n",
      "🚫 Image 14604.jpg has no annotations.\n",
      "🚫 Image 10172.jpg has no annotations.\n",
      "🚫 Image 8096.jpg has no annotations.\n",
      "🚫 Image 6754.jpg has no annotations.\n",
      "🚫 Image 10684.jpg has no annotations.\n",
      "🚫 Image 19182.jpg has no annotations.\n",
      "🚫 Image 13946.jpg has no annotations.\n",
      "🚫 Image 3060.jpg has no annotations.\n",
      "🚫 Image 259.jpg has no annotations.\n",
      "🚫 Image 12208.jpg has no annotations.\n",
      "🚫 Image 14838.jpg has no annotations.\n",
      "🚫 Image 413.jpg has no annotations.\n",
      "🚫 Image 9598.jpg has no annotations.\n",
      "🚫 Image 5377.jpg has no annotations.\n",
      "🚫 Image 18365.jpg has no annotations.\n",
      "🚫 Image 6001.jpg has no annotations.\n",
      "🚫 Image 17086.jpg has no annotations.\n",
      "🚫 Image 15855.jpg has no annotations.\n",
      "🚫 Image 13214.jpg has no annotations.\n",
      "🚫 Image 10157.jpg has no annotations.\n",
      "🚫 Image 4145.jpg has no annotations.\n",
      "🚫 Image 18920.jpg has no annotations.\n",
      "🚫 Image 111.jpg has no annotations.\n",
      "🚫 Image 7840.jpg has no annotations.\n",
      "🚫 Image 11074.jpg has no annotations.\n",
      "🚫 Image 12684.jpg has no annotations.\n",
      "🚫 Image 16685.jpg has no annotations.\n",
      "🚫 Image 8554.jpg has no annotations.\n",
      "🚫 Image 17916.jpg has no annotations.\n",
      "🚫 Image 11741.jpg has no annotations.\n",
      "🚫 Image 8008.jpg has no annotations.\n",
      "🚫 Image 1917.jpg has no annotations.\n",
      "🚫 Image 17134.jpg has no annotations.\n",
      "🚫 Image 10735.jpg has no annotations.\n",
      "🚫 Image 3500.jpg has no annotations.\n",
      "🚫 Image 17185.jpg has no annotations.\n",
      "🚫 Image 3134.jpg has no annotations.\n",
      "🚫 Image 11408.jpg has no annotations.\n",
      "🚫 Image 5527.jpg has no annotations.\n",
      "🚫 Image 8103.jpg has no annotations.\n",
      "🚫 Image 2178.jpg has no annotations.\n",
      "🚫 Image 11373.jpg has no annotations.\n",
      "🚫 Image 673.jpg has no annotations.\n",
      "🚫 Image 5867.jpg has no annotations.\n",
      "🚫 Image 5262.jpg has no annotations.\n",
      "🚫 Image 10009.jpg has no annotations.\n",
      "🚫 Image 11896.jpg has no annotations.\n",
      "🚫 Image 11105.jpg has no annotations.\n",
      "🚫 Image 8990.jpg has no annotations.\n",
      "🚫 Image 15273.jpg has no annotations.\n",
      "🚫 Image 10269.jpg has no annotations.\n",
      "🚫 Image 12699.jpg has no annotations.\n",
      "🚫 Image 17691.jpg has no annotations.\n",
      "🚫 Image 9056.jpg has no annotations.\n",
      "🚫 Image 1458.jpg has no annotations.\n",
      "🚫 Image 19915.jpg has no annotations.\n",
      "🚫 Image 1281.jpg has no annotations.\n",
      "🚫 Image 16610.jpg has no annotations.\n",
      "\n",
      "📊 Summary:\n",
      "🖼️  Total images: 20000\n",
      "✅ Images with at least one valid annotation: 19253\n",
      "🚫 Images with no valid annotations: 747\n",
      "❌ Skipped annotations: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_coco_annotations(\"./json_dataset/annotations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a94c1",
   "metadata": {},
   "source": [
    "Changing the function to include negative examples of empty images (0 objects inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcd97d8-81b7-4e87-880a-cdccac70e198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from ultralytics.utils import DATASETS_DIR, LOGGER, NUM_THREADS, TQDM\n",
    "from ultralytics.utils.downloads import download, zip_directory\n",
    "from ultralytics.utils.files import increment_path\n",
    "\n",
    "\n",
    "def coco91_to_coco80_class():\n",
    "    \"\"\"\n",
    "    Converts 91-index COCO class IDs to 80-index COCO class IDs.\n",
    "\n",
    "    Returns:\n",
    "        (list): A list of 91 class IDs where the index represents the 80-index class ID and the value is the\n",
    "            corresponding 91-index class ID.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8,\n",
    "        9,\n",
    "        10,\n",
    "        None,\n",
    "        11,\n",
    "        12,\n",
    "        13,\n",
    "        14,\n",
    "        15,\n",
    "        16,\n",
    "        17,\n",
    "        18,\n",
    "        19,\n",
    "        20,\n",
    "        21,\n",
    "        22,\n",
    "        23,\n",
    "        None,\n",
    "        24,\n",
    "        25,\n",
    "        None,\n",
    "        None,\n",
    "        26,\n",
    "        27,\n",
    "        28,\n",
    "        29,\n",
    "        30,\n",
    "        31,\n",
    "        32,\n",
    "        33,\n",
    "        34,\n",
    "        35,\n",
    "        36,\n",
    "        37,\n",
    "        38,\n",
    "        39,\n",
    "        None,\n",
    "        40,\n",
    "        41,\n",
    "        42,\n",
    "        43,\n",
    "        44,\n",
    "        45,\n",
    "        46,\n",
    "        47,\n",
    "        48,\n",
    "        49,\n",
    "        50,\n",
    "        51,\n",
    "        52,\n",
    "        53,\n",
    "        54,\n",
    "        55,\n",
    "        56,\n",
    "        57,\n",
    "        58,\n",
    "        59,\n",
    "        None,\n",
    "        60,\n",
    "        None,\n",
    "        None,\n",
    "        61,\n",
    "        None,\n",
    "        62,\n",
    "        63,\n",
    "        64,\n",
    "        65,\n",
    "        66,\n",
    "        67,\n",
    "        68,\n",
    "        69,\n",
    "        70,\n",
    "        71,\n",
    "        72,\n",
    "        None,\n",
    "        73,\n",
    "        74,\n",
    "        75,\n",
    "        76,\n",
    "        77,\n",
    "        78,\n",
    "        79,\n",
    "        None,\n",
    "    ]\n",
    "\n",
    "\n",
    "def coco80_to_coco91_class():\n",
    "    r\"\"\"\n",
    "    Converts 80-index (val2014) to 91-index (paper).\n",
    "    For details see https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/.\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.loadtxt(\"data/coco.names\", dtype=\"str\", delimiter=\"\\n\")\n",
    "        >>> b = np.loadtxt(\"data/coco_paper.names\", dtype=\"str\", delimiter=\"\\n\")\n",
    "\n",
    "        Convert the darknet to COCO format\n",
    "        >>> x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]\n",
    "\n",
    "        Convert the COCO to darknet format\n",
    "        >>> x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8,\n",
    "        9,\n",
    "        10,\n",
    "        11,\n",
    "        13,\n",
    "        14,\n",
    "        15,\n",
    "        16,\n",
    "        17,\n",
    "        18,\n",
    "        19,\n",
    "        20,\n",
    "        21,\n",
    "        22,\n",
    "        23,\n",
    "        24,\n",
    "        25,\n",
    "        27,\n",
    "        28,\n",
    "        31,\n",
    "        32,\n",
    "        33,\n",
    "        34,\n",
    "        35,\n",
    "        36,\n",
    "        37,\n",
    "        38,\n",
    "        39,\n",
    "        40,\n",
    "        41,\n",
    "        42,\n",
    "        43,\n",
    "        44,\n",
    "        46,\n",
    "        47,\n",
    "        48,\n",
    "        49,\n",
    "        50,\n",
    "        51,\n",
    "        52,\n",
    "        53,\n",
    "        54,\n",
    "        55,\n",
    "        56,\n",
    "        57,\n",
    "        58,\n",
    "        59,\n",
    "        60,\n",
    "        61,\n",
    "        62,\n",
    "        63,\n",
    "        64,\n",
    "        65,\n",
    "        67,\n",
    "        70,\n",
    "        72,\n",
    "        73,\n",
    "        74,\n",
    "        75,\n",
    "        76,\n",
    "        77,\n",
    "        78,\n",
    "        79,\n",
    "        80,\n",
    "        81,\n",
    "        82,\n",
    "        84,\n",
    "        85,\n",
    "        86,\n",
    "        87,\n",
    "        88,\n",
    "        89,\n",
    "        90,\n",
    "    ]\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from ultralytics.utils import LOGGER\n",
    "from ultralytics.utils.checks import check_requirements\n",
    "from ultralytics.utils.files import increment_path\n",
    "from ultralytics.data.converter import coco91_to_coco80_class, merge_multi_segment\n",
    "from tqdm import tqdm as TQDM\n",
    "\n",
    "\n",
    "def convert_coco(\n",
    "    labels_dir=\"../coco/annotations/\",\n",
    "    save_dir=\"coco_converted/\",\n",
    "    use_segments=False,\n",
    "    use_keypoints=False,\n",
    "    cls91to80=True,\n",
    "    lvis=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts COCO dataset annotations to a YOLO annotation format suitable for training YOLO models.\n",
    "\n",
    "    Args:\n",
    "        labels_dir (str, optional): Path to directory containing COCO dataset annotation files.\n",
    "        save_dir (str, optional): Path to directory to save results to.\n",
    "        use_segments (bool, optional): Whether to include segmentation masks in the output.\n",
    "        use_keypoints (bool, optional): Whether to include keypoint annotations in the output.\n",
    "        cls91to80 (bool, optional): Whether to map 91 COCO class IDs to the corresponding 80 COCO class IDs.\n",
    "        lvis (bool, optional): Whether to convert data in LVIS dataset format.\n",
    "\n",
    "    Examples:\n",
    "        >>> convert_coco(\"../datasets/coco/annotations/\", use_segments=True, use_keypoints=False, cls91to80=False)\n",
    "        >>> convert_coco(\"../datasets/lvis/annotations/\", use_segments=True, lvis=True)\n",
    "    \"\"\"\n",
    "    # Create dataset directory\n",
    "    save_dir = increment_path(save_dir)\n",
    "    for p in save_dir / \"labels\", save_dir / \"images\":\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Convert classes\n",
    "    coco80 = coco91_to_coco80_class()\n",
    "\n",
    "    # Process each JSON file\n",
    "    for json_file in sorted(Path(labels_dir).resolve().glob(\"*.json\")):\n",
    "        lname = \"\" if lvis else json_file.stem.replace(\"instances_\", \"\")\n",
    "        fn = Path(save_dir) / \"labels\" / lname\n",
    "        fn.mkdir(parents=True, exist_ok=True)\n",
    "        if lvis:\n",
    "            (fn / \"train2017\").mkdir(parents=True, exist_ok=True)\n",
    "            (fn / \"val2017\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(json_file, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        images = {f\"{x['id']:d}\": x for x in data[\"images\"]}\n",
    "        annotations = defaultdict(list)\n",
    "        for ann in data[\"annotations\"]:\n",
    "            annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "        image_txt = []\n",
    "\n",
    "        # Process each image, even if it has no annotations\n",
    "        for img_id, img in TQDM(images.items(), desc=f\"Images {json_file}\"):\n",
    "            anns = annotations.get(int(img_id), [])\n",
    "            h, w = img[\"height\"], img[\"width\"]\n",
    "            f = str(Path(img[\"coco_url\"]).relative_to(\"http://images.cocodataset.org\")) if lvis else img[\"file_name\"]\n",
    "            if lvis:\n",
    "                image_txt.append(str(Path(\"./images\") / f))\n",
    "\n",
    "            bboxes = []\n",
    "            segments = []\n",
    "            keypoints = []\n",
    "\n",
    "            for ann in anns:\n",
    "                if ann.get(\"iscrowd\", False):\n",
    "                    continue\n",
    "                box = np.array(ann[\"bbox\"], dtype=np.float64)\n",
    "                box[:2] += box[2:] / 2  # convert to center x/y\n",
    "                box[[0, 2]] /= w\n",
    "                box[[1, 3]] /= h\n",
    "                if box[2] <= 0 or box[3] <= 0:\n",
    "                    continue\n",
    "\n",
    "                cls = coco80[ann[\"category_id\"] - 1] if cls91to80 else ann[\"category_id\"]\n",
    "                box = [cls] + box.tolist()\n",
    "                if box not in bboxes:\n",
    "                    bboxes.append(box)\n",
    "                    if use_segments and ann.get(\"segmentation\") is not None:\n",
    "                        if len(ann[\"segmentation\"]) == 0:\n",
    "                            segments.append([])\n",
    "                            continue\n",
    "                        elif len(ann[\"segmentation\"]) > 1:\n",
    "                            s = merge_multi_segment(ann[\"segmentation\"])\n",
    "                            s = (np.concatenate(s, axis=0) / np.array([w, h])).reshape(-1).tolist()\n",
    "                        else:\n",
    "                            s = [j for i in ann[\"segmentation\"] for j in i]\n",
    "                            s = (np.array(s).reshape(-1, 2) / np.array([w, h])).reshape(-1).tolist()\n",
    "                        s = [cls] + s\n",
    "                        segments.append(s)\n",
    "                    if use_keypoints and ann.get(\"keypoints\") is not None:\n",
    "                        keypoints.append(\n",
    "                            box + (np.array(ann[\"keypoints\"]).reshape(-1, 3) / np.array([w, h, 1])).reshape(-1).tolist()\n",
    "                        )\n",
    "\n",
    "            # Write label file (even if empty)\n",
    "            label_path = (fn / f).with_suffix(\".txt\")\n",
    "            with open(label_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for i in range(len(bboxes)):\n",
    "                    if use_keypoints:\n",
    "                        line = (*(keypoints[i]),)\n",
    "                    else:\n",
    "                        line = (*(segments[i] if use_segments and len(segments[i]) > 0 else bboxes[i]),)\n",
    "                    file.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n",
    "\n",
    "        if lvis:\n",
    "            filename = Path(save_dir) / json_file.name.replace(\"lvis_v1_\", \"\").replace(\".json\", \".txt\")\n",
    "            with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.writelines(f\"{line}\\n\" for line in image_txt)\n",
    "\n",
    "    LOGGER.info(f\"{'LVIS' if lvis else 'COCO'} data converted successfully.\\nResults saved to {save_dir.resolve()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_segment_masks_to_yolo_seg(masks_dir, output_dir, classes):\n",
    "    \"\"\"\n",
    "    Converts a dataset of segmentation mask images to the YOLO segmentation format.\n",
    "\n",
    "    This function takes the directory containing the binary format mask images and converts them into YOLO segmentation format.\n",
    "    The converted masks are saved in the specified output directory.\n",
    "\n",
    "    Args:\n",
    "        masks_dir (str): The path to the directory where all mask images (png, jpg) are stored.\n",
    "        output_dir (str): The path to the directory where the converted YOLO segmentation masks will be stored.\n",
    "        classes (int): Total classes in the dataset i.e. for COCO classes=80\n",
    "\n",
    "    Examples:\n",
    "        >>> from ultralytics.data.converter import convert_segment_masks_to_yolo_seg\n",
    "\n",
    "        The classes here is the total classes in the dataset, for COCO dataset we have 80 classes\n",
    "        >>> convert_segment_masks_to_yolo_seg(\"path/to/masks_directory\", \"path/to/output/directory\", classes=80)\n",
    "\n",
    "    Notes:\n",
    "        The expected directory structure for the masks is:\n",
    "\n",
    "            - masks\n",
    "                ├─ mask_image_01.png or mask_image_01.jpg\n",
    "                ├─ mask_image_02.png or mask_image_02.jpg\n",
    "                ├─ mask_image_03.png or mask_image_03.jpg\n",
    "                └─ mask_image_04.png or mask_image_04.jpg\n",
    "\n",
    "        After execution, the labels will be organized in the following structure:\n",
    "\n",
    "            - output_dir\n",
    "                ├─ mask_yolo_01.txt\n",
    "                ├─ mask_yolo_02.txt\n",
    "                ├─ mask_yolo_03.txt\n",
    "                └─ mask_yolo_04.txt\n",
    "    \"\"\"\n",
    "    pixel_to_class_mapping = {i + 1: i for i in range(classes)}\n",
    "    for mask_path in Path(masks_dir).iterdir():\n",
    "        if mask_path.suffix in {\".png\", \".jpg\"}:\n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)  # Read the mask image in grayscale\n",
    "            img_height, img_width = mask.shape  # Get image dimensions\n",
    "            LOGGER.info(f\"Processing {mask_path} imgsz = {img_height} x {img_width}\")\n",
    "\n",
    "            unique_values = np.unique(mask)  # Get unique pixel values representing different classes\n",
    "            yolo_format_data = []\n",
    "\n",
    "            for value in unique_values:\n",
    "                if value == 0:\n",
    "                    continue  # Skip background\n",
    "                class_index = pixel_to_class_mapping.get(value, -1)\n",
    "                if class_index == -1:\n",
    "                    LOGGER.warning(f\"Unknown class for pixel value {value} in file {mask_path}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Create a binary mask for the current class and find contours\n",
    "                contours, _ = cv2.findContours(\n",
    "                    (mask == value).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "                )  # Find contours\n",
    "\n",
    "                for contour in contours:\n",
    "                    if len(contour) >= 3:  # YOLO requires at least 3 points for a valid segmentation\n",
    "                        contour = contour.squeeze()  # Remove single-dimensional entries\n",
    "                        yolo_format = [class_index]\n",
    "                        for point in contour:\n",
    "                            # Normalize the coordinates\n",
    "                            yolo_format.append(round(point[0] / img_width, 6))  # Rounding to 6 decimal places\n",
    "                            yolo_format.append(round(point[1] / img_height, 6))\n",
    "                        yolo_format_data.append(yolo_format)\n",
    "            # Save Ultralytics YOLO format data to file\n",
    "            output_path = Path(output_dir) / f\"{mask_path.stem}.txt\"\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for item in yolo_format_data:\n",
    "                    line = \" \".join(map(str, item))\n",
    "                    file.write(line + \"\\n\")\n",
    "            LOGGER.info(f\"Processed and stored at {output_path} imgsz = {img_height} x {img_width}\")\n",
    "\n",
    "\n",
    "def convert_dota_to_yolo_obb(dota_root_path: str):\n",
    "    \"\"\"\n",
    "    Converts DOTA dataset annotations to YOLO OBB (Oriented Bounding Box) format.\n",
    "\n",
    "    The function processes images in the 'train' and 'val' folders of the DOTA dataset. For each image, it reads the\n",
    "    associated label from the original labels directory and writes new labels in YOLO OBB format to a new directory.\n",
    "\n",
    "    Args:\n",
    "        dota_root_path (str): The root directory path of the DOTA dataset.\n",
    "\n",
    "    Examples:\n",
    "        >>> from ultralytics.data.converter import convert_dota_to_yolo_obb\n",
    "        >>> convert_dota_to_yolo_obb(\"path/to/DOTA\")\n",
    "\n",
    "    Notes:\n",
    "        The directory structure assumed for the DOTA dataset:\n",
    "\n",
    "            - DOTA\n",
    "                ├─ images\n",
    "                │   ├─ train\n",
    "                │   └─ val\n",
    "                └─ labels\n",
    "                    ├─ train_original\n",
    "                    └─ val_original\n",
    "\n",
    "        After execution, the function will organize the labels into:\n",
    "\n",
    "            - DOTA\n",
    "                └─ labels\n",
    "                    ├─ train\n",
    "                    └─ val\n",
    "    \"\"\"\n",
    "    dota_root_path = Path(dota_root_path)\n",
    "\n",
    "    # Class names to indices mapping\n",
    "    class_mapping = {\n",
    "        \"plane\": 0,\n",
    "        \"ship\": 1,\n",
    "        \"storage-tank\": 2,\n",
    "        \"baseball-diamond\": 3,\n",
    "        \"tennis-court\": 4,\n",
    "        \"basketball-court\": 5,\n",
    "        \"ground-track-field\": 6,\n",
    "        \"harbor\": 7,\n",
    "        \"bridge\": 8,\n",
    "        \"large-vehicle\": 9,\n",
    "        \"small-vehicle\": 10,\n",
    "        \"helicopter\": 11,\n",
    "        \"roundabout\": 12,\n",
    "        \"soccer-ball-field\": 13,\n",
    "        \"swimming-pool\": 14,\n",
    "        \"container-crane\": 15,\n",
    "        \"airport\": 16,\n",
    "        \"helipad\": 17,\n",
    "    }\n",
    "\n",
    "    def convert_label(image_name, image_width, image_height, orig_label_dir, save_dir):\n",
    "        \"\"\"Converts a single image's DOTA annotation to YOLO OBB format and saves it to a specified directory.\"\"\"\n",
    "        orig_label_path = orig_label_dir / f\"{image_name}.txt\"\n",
    "        save_path = save_dir / f\"{image_name}.txt\"\n",
    "\n",
    "        with orig_label_path.open(\"r\") as f, save_path.open(\"w\") as g:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 9:\n",
    "                    continue\n",
    "                class_name = parts[8]\n",
    "                class_idx = class_mapping[class_name]\n",
    "                coords = [float(p) for p in parts[:8]]\n",
    "                normalized_coords = [\n",
    "                    coords[i] / image_width if i % 2 == 0 else coords[i] / image_height for i in range(8)\n",
    "                ]\n",
    "                formatted_coords = [f\"{coord:.6g}\" for coord in normalized_coords]\n",
    "                g.write(f\"{class_idx} {' '.join(formatted_coords)}\\n\")\n",
    "\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        image_dir = dota_root_path / \"images\" / phase\n",
    "        orig_label_dir = dota_root_path / \"labels\" / f\"{phase}_original\"\n",
    "        save_dir = dota_root_path / \"labels\" / phase\n",
    "\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        image_paths = list(image_dir.iterdir())\n",
    "        for image_path in TQDM(image_paths, desc=f\"Processing {phase} images\"):\n",
    "            if image_path.suffix != \".png\":\n",
    "                continue\n",
    "            image_name_without_ext = image_path.stem\n",
    "            img = cv2.imread(str(image_path))\n",
    "            h, w = img.shape[:2]\n",
    "            convert_label(image_name_without_ext, w, h, orig_label_dir, save_dir)\n",
    "\n",
    "\n",
    "def min_index(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Find a pair of indexes with the shortest distance between two arrays of 2D points.\n",
    "\n",
    "    Args:\n",
    "        arr1 (np.ndarray): A NumPy array of shape (N, 2) representing N 2D points.\n",
    "        arr2 (np.ndarray): A NumPy array of shape (M, 2) representing M 2D points.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): A tuple containing the indexes of the points with the shortest distance in arr1 and arr2 respectively.\n",
    "    \"\"\"\n",
    "    dis = ((arr1[:, None, :] - arr2[None, :, :]) ** 2).sum(-1)\n",
    "    return np.unravel_index(np.argmin(dis, axis=None), dis.shape)\n",
    "\n",
    "\n",
    "def merge_multi_segment(segments):\n",
    "    \"\"\"\n",
    "    Merge multiple segments into one list by connecting the coordinates with the minimum distance between each segment.\n",
    "    This function connects these coordinates with a thin line to merge all segments into one.\n",
    "\n",
    "    Args:\n",
    "        segments (List[List]): Original segmentations in COCO's JSON file.\n",
    "                               Each element is a list of coordinates, like [segmentation1, segmentation2,...].\n",
    "\n",
    "    Returns:\n",
    "        s (List[np.ndarray]): A list of connected segments represented as NumPy arrays.\n",
    "    \"\"\"\n",
    "    s = []\n",
    "    segments = [np.array(i).reshape(-1, 2) for i in segments]\n",
    "    idx_list = [[] for _ in range(len(segments))]\n",
    "\n",
    "    # Record the indexes with min distance between each segment\n",
    "    for i in range(1, len(segments)):\n",
    "        idx1, idx2 = min_index(segments[i - 1], segments[i])\n",
    "        idx_list[i - 1].append(idx1)\n",
    "        idx_list[i].append(idx2)\n",
    "\n",
    "    # Use two round to connect all the segments\n",
    "    for k in range(2):\n",
    "        # Forward connection\n",
    "        if k == 0:\n",
    "            for i, idx in enumerate(idx_list):\n",
    "                # Middle segments have two indexes, reverse the index of middle segments\n",
    "                if len(idx) == 2 and idx[0] > idx[1]:\n",
    "                    idx = idx[::-1]\n",
    "                    segments[i] = segments[i][::-1, :]\n",
    "\n",
    "                segments[i] = np.roll(segments[i], -idx[0], axis=0)\n",
    "                segments[i] = np.concatenate([segments[i], segments[i][:1]])\n",
    "                # Deal with the first segment and the last one\n",
    "                if i in {0, len(idx_list) - 1}:\n",
    "                    s.append(segments[i])\n",
    "                else:\n",
    "                    idx = [0, idx[1] - idx[0]]\n",
    "                    s.append(segments[i][idx[0] : idx[1] + 1])\n",
    "\n",
    "        else:\n",
    "            for i in range(len(idx_list) - 1, -1, -1):\n",
    "                if i not in {0, len(idx_list) - 1}:\n",
    "                    idx = idx_list[i]\n",
    "                    nidx = abs(idx[1] - idx[0])\n",
    "                    s.append(segments[i][nidx:])\n",
    "    return s\n",
    "\n",
    "\n",
    "def yolo_bbox2segment(im_dir, save_dir=None, sam_model=\"sam_b.pt\", device=None):\n",
    "    \"\"\"\n",
    "    Converts existing object detection dataset (bounding boxes) to segmentation dataset or oriented bounding box (OBB)\n",
    "    in YOLO format. Generates segmentation data using SAM auto-annotator as needed.\n",
    "\n",
    "    Args:\n",
    "        im_dir (str | Path): Path to image directory to convert.\n",
    "        save_dir (str | Path): Path to save the generated labels, labels will be saved\n",
    "            into `labels-segment` in the same directory level of `im_dir` if save_dir is None.\n",
    "        sam_model (str): Segmentation model to use for intermediate segmentation data.\n",
    "        device (int | str): The specific device to run SAM models.\n",
    "\n",
    "    Notes:\n",
    "        The input directory structure assumed for dataset:\n",
    "\n",
    "            - im_dir\n",
    "                ├─ 001.jpg\n",
    "                ├─ ...\n",
    "                └─ NNN.jpg\n",
    "            - labels\n",
    "                ├─ 001.txt\n",
    "                ├─ ...\n",
    "                └─ NNN.txt\n",
    "    \"\"\"\n",
    "    from ultralytics import SAM\n",
    "    from ultralytics.data import YOLODataset\n",
    "    from ultralytics.utils.ops import xywh2xyxy\n",
    "\n",
    "    # NOTE: add placeholder to pass class index check\n",
    "    dataset = YOLODataset(im_dir, data=dict(names=list(range(1000))))\n",
    "    if len(dataset.labels[0][\"segments\"]) > 0:  # if it's segment data\n",
    "        LOGGER.info(\"Segmentation labels detected, no need to generate new ones!\")\n",
    "        return\n",
    "\n",
    "    LOGGER.info(\"Detection labels detected, generating segment labels by SAM model!\")\n",
    "    sam_model = SAM(sam_model)\n",
    "    for label in TQDM(dataset.labels, total=len(dataset.labels), desc=\"Generating segment labels\"):\n",
    "        h, w = label[\"shape\"]\n",
    "        boxes = label[\"bboxes\"]\n",
    "        if len(boxes) == 0:  # skip empty labels\n",
    "            continue\n",
    "        boxes[:, [0, 2]] *= w\n",
    "        boxes[:, [1, 3]] *= h\n",
    "        im = cv2.imread(label[\"im_file\"])\n",
    "        sam_results = sam_model(im, bboxes=xywh2xyxy(boxes), verbose=False, save=False, device=device)\n",
    "        label[\"segments\"] = sam_results[0].masks.xyn\n",
    "\n",
    "    save_dir = Path(save_dir) if save_dir else Path(im_dir).parent / \"labels-segment\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for label in dataset.labels:\n",
    "        texts = []\n",
    "        lb_name = Path(label[\"im_file\"]).with_suffix(\".txt\").name\n",
    "        txt_file = save_dir / lb_name\n",
    "        cls = label[\"cls\"]\n",
    "        for i, s in enumerate(label[\"segments\"]):\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            line = (int(cls[i]), *s.reshape(-1))\n",
    "            texts.append((\"%g \" * len(line)).rstrip() % line)\n",
    "        with open(txt_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(text + \"\\n\" for text in texts)\n",
    "    LOGGER.info(f\"Generated segment labels saved in {save_dir}\")\n",
    "\n",
    "\n",
    "def create_synthetic_coco_dataset():\n",
    "    \"\"\"\n",
    "    Creates a synthetic COCO dataset with random images based on filenames from label lists.\n",
    "\n",
    "    This function downloads COCO labels, reads image filenames from label list files,\n",
    "    creates synthetic images for train2017 and val2017 subsets, and organizes\n",
    "    them in the COCO dataset structure. It uses multithreading to generate images efficiently.\n",
    "\n",
    "    Examples:\n",
    "        >>> from ultralytics.data.converter import create_synthetic_coco_dataset\n",
    "        >>> create_synthetic_coco_dataset()\n",
    "\n",
    "    Notes:\n",
    "        - Requires internet connection to download label files.\n",
    "        - Generates random RGB images of varying sizes (480x480 to 640x640 pixels).\n",
    "        - Existing test2017 directory is removed as it's not needed.\n",
    "        - Reads image filenames from train2017.txt and val2017.txt files.\n",
    "    \"\"\"\n",
    "\n",
    "    def create_synthetic_image(image_file):\n",
    "        \"\"\"Generates synthetic images with random sizes and colors for dataset augmentation or testing purposes.\"\"\"\n",
    "        if not image_file.exists():\n",
    "            size = (random.randint(480, 640), random.randint(480, 640))\n",
    "            Image.new(\n",
    "                \"RGB\",\n",
    "                size=size,\n",
    "                color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)),\n",
    "            ).save(image_file)\n",
    "\n",
    "    # Download labels\n",
    "    dir = DATASETS_DIR / \"coco\"\n",
    "    url = \"https://github.com/ultralytics/assets/releases/download/v0.0.0/\"\n",
    "    label_zip = \"coco2017labels-segments.zip\"\n",
    "    download([url + label_zip], dir=dir.parent)\n",
    "\n",
    "    # Create synthetic images\n",
    "    shutil.rmtree(dir / \"labels\" / \"test2017\", ignore_errors=True)  # Remove test2017 directory as not needed\n",
    "    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "        for subset in [\"train2017\", \"val2017\"]:\n",
    "            subset_dir = dir / \"images\" / subset\n",
    "            subset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Read image filenames from label list file\n",
    "            label_list_file = dir / f\"{subset}.txt\"\n",
    "            if label_list_file.exists():\n",
    "                with open(label_list_file, encoding=\"utf-8\") as f:\n",
    "                    image_files = [dir / line.strip() for line in f]\n",
    "\n",
    "                # Submit all tasks\n",
    "                futures = [executor.submit(create_synthetic_image, image_file) for image_file in image_files]\n",
    "                for _ in TQDM(as_completed(futures), total=len(futures), desc=f\"Generating images for {subset}\"):\n",
    "                    pass  # The actual work is done in the background\n",
    "            else:\n",
    "                LOGGER.warning(f\"Labels file {label_list_file} does not exist. Skipping image creation for {subset}.\")\n",
    "\n",
    "    LOGGER.info(\"Synthetic COCO dataset created successfully.\")\n",
    "\n",
    "\n",
    "def convert_to_multispectral(path, n_channels=10, replace=False, zip=False):\n",
    "    \"\"\"\n",
    "    Convert RGB images to multispectral images by interpolating across wavelength bands.\n",
    "\n",
    "    This function takes RGB images and interpolates them to create multispectral images with a specified number\n",
    "    of channels. It can process either a single image or a directory of images.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to an image file or directory containing images to convert.\n",
    "        n_channels (int): Number of spectral channels to generate in the output image.\n",
    "        replace (bool): Whether to replace the original image file with the converted one.\n",
    "        zip (bool): Whether to zip the converted images into a zip file.\n",
    "\n",
    "    Examples:\n",
    "        >>> # Convert a single image\n",
    "        >>> convert_to_multispectral(\"path/to/image.jpg\", n_channels=10)\n",
    "        >>> # Convert a dataset\n",
    "        >>> convert_to_multispectral(\"../datasets/coco8\", n_channels=10)\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import interp1d\n",
    "\n",
    "    from ultralytics.data.utils import IMG_FORMATS\n",
    "\n",
    "    path = Path(path)\n",
    "    if path.is_dir():\n",
    "        # Process directory\n",
    "        im_files = sum([list(path.rglob(f\"*.{ext}\")) for ext in (IMG_FORMATS - {\"tif\", \"tiff\"})], [])\n",
    "        for im_path in im_files:\n",
    "            try:\n",
    "                convert_to_multispectral(im_path, n_channels)\n",
    "                if replace:\n",
    "                    im_path.unlink()\n",
    "            except Exception as e:\n",
    "                LOGGER.info(f\"Error converting {im_path}: {e}\")\n",
    "\n",
    "        if zip:\n",
    "            zip_directory(path)\n",
    "    else:\n",
    "        # Process a single image\n",
    "        output_path = path.with_suffix(\".tiff\")\n",
    "        img = cv2.cvtColor(cv2.imread(str(path)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Interpolate all pixels at once\n",
    "        rgb_wavelengths = np.array([650, 510, 475])  # R, G, B wavelengths (nm)\n",
    "        target_wavelengths = np.linspace(450, 700, n_channels)\n",
    "        f = interp1d(rgb_wavelengths.T, img, kind=\"linear\", bounds_error=False, fill_value=\"extrapolate\")\n",
    "        multispectral = f(target_wavelengths)\n",
    "        cv2.imwritemulti(str(output_path), np.clip(multispectral, 0, 255).astype(np.uint8).transpose(2, 0, 1))\n",
    "        LOGGER.info(f\"Converted {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e413b857-dd77-4b60-8a44-06348451d6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images /home/jupyter/til25-import-torch/cv/json_dataset/annotations.json: 100%|██████████| 20000/20000 [00:02<00:00, 6765.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data converted successfully.\n",
      "Results saved to /home/jupyter/til25-import-torch/cv/yaml_dataset2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_coco(\n",
    "    labels_dir=\"/home/jupyter/til25-import-torch/cv/json_dataset\",  # Directory containing your json file\n",
    "    save_dir=\"/home/jupyter/til25-import-torch/cv/yaml_dataset\",\n",
    "    use_keypoints=False,  # Since you're using keypoints data\n",
    "    use_segments=False,\n",
    "    cls91to80=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6a957c-e2ec-4997-bffb-cc26e229f560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified split complete: 15999 train, 4001 val\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Paths\n",
    "images_src = Path(\"/home/jupyter/til25-import-torch/cv/shrinked_dataset/images\")\n",
    "labels_src = Path(\"/home/jupyter/til25-import-torch/cv/shrinked_dataset/labels/\")\n",
    "dst_base = Path(\"/home/jupyter/til25-import-torch/cv/aug_dataset_split\")\n",
    "\n",
    "# Make sure output folders exist\n",
    "for split in ['train/images', 'train/labels', 'val/images', 'val/labels']:\n",
    "    (dst_base / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Gather image files and determine dominant class for stratification\n",
    "image_paths = list(images_src.glob(\"*.jpg\")) + list(images_src.glob(\"*.png\")) + list(images_src.glob(\"*.jpeg\"))\n",
    "\n",
    "labeled_image_names = []\n",
    "labeled_class_labels = []\n",
    "unlabeled_image_names = []\n",
    "\n",
    "for img_path in image_paths:\n",
    "    label_file = labels_src / (img_path.stem + \".txt\")\n",
    "    if not label_file.exists():\n",
    "        continue  # skip images without any label file\n",
    "\n",
    "    try:\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        if not lines:\n",
    "            # Empty label file, treat as unlabeled image\n",
    "            unlabeled_image_names.append(img_path.name)\n",
    "        else:\n",
    "            class_ids = [int(line.split()[0]) for line in lines]\n",
    "            dominant_class = Counter(class_ids).most_common(1)[0][0]\n",
    "            labeled_image_names.append(img_path.name)\n",
    "            labeled_class_labels.append(dominant_class)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {label_file}: {e}\")\n",
    "\n",
    "# Stratified split for labeled images\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(labeled_image_names, labeled_class_labels))\n",
    "\n",
    "train_images = [labeled_image_names[i] for i in train_idx]\n",
    "val_images = [labeled_image_names[i] for i in val_idx]\n",
    "\n",
    "# Random split for unlabeled images\n",
    "random.seed(42)\n",
    "random.shuffle(unlabeled_image_names)\n",
    "split_point = int(0.8 * len(unlabeled_image_names))\n",
    "train_images += unlabeled_image_names[:split_point]\n",
    "val_images += unlabeled_image_names[split_point:]\n",
    "\n",
    "# Helper to symlink image + label\n",
    "def symlink_subset(image_list, split):\n",
    "    for img_name in image_list:\n",
    "        label_name = img_name.rsplit('.', 1)[0] + \".txt\"\n",
    "\n",
    "        img_src = images_src / img_name\n",
    "        lbl_src = labels_src / label_name\n",
    "\n",
    "        img_dst = dst_base / split / \"images\" / img_name\n",
    "        lbl_dst = dst_base / split / \"labels\" / label_name\n",
    "\n",
    "        try:\n",
    "            os.symlink(img_src.resolve(), img_dst)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        # Symlink label file even if empty\n",
    "        if lbl_src.exists():\n",
    "            try:\n",
    "                os.symlink(lbl_src.resolve(), lbl_dst)\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "\n",
    "# Create symlinks for train and val sets\n",
    "symlink_subset(train_images, \"train\")\n",
    "symlink_subset(val_images, \"val\")\n",
    "\n",
    "print(f\"Stratified split complete: {len(train_images)} train, {len(val_images)} val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef14965-e2bf-4c40-b25d-160256e59556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cvenv",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "CVenv",
   "language": "python",
   "name": "cvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
